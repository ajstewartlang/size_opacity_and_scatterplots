---
format: acm-pdf

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
keep-tex: true

bibliography: size-contrast-new.bib

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

title: Further Investigating the Effects of Point Size and Contrast on Correlation Perception in Scatterplots

# if short-title is defined, then it's used
short-title: Size, Contrast, and Scatterplots

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: manuscript, review, anonymous, screen  

  # acm preamble information
  copyright-year: 2018
  acm-year: 2018
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation emai
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
     <concept>
      <concept_id>10010520.10010553.10010562</concept_id>
      <concept_desc>Computer systems organization~Embedded systems</concept_desc>
      <concept_significance>500</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010575.10010755</concept_id>
      <concept_desc>Computer systems organization~Redundancy</concept_desc>
      <concept_significance>300</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010553.10010554</concept_id>
      <concept_desc>Computer systems organization~Robotics</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
     <concept>
      <concept_id>10003033.10003083.10003095</concept_id>
      <concept_desc>Networks~Network reliability</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
    </ccs2012>
    \end{CCSXML}
    
    \ccsdesc[500]{Computer systems organization~Embedded systems}
    \ccsdesc[300]{Computer systems organization~Redundancy}
    \ccsdesc{Computer systems organization~Robotics}
    \ccsdesc[100]{Networks~Network reliability}

  keywords:
    - correlation
    - scatterplot
    - perception
    - crowdsourced

abstract: |
  It is paramount that the toolkit of the data visualization designer must 
  continue to grow based on robust, reproducible experimental work. 
  To this effect, we present a single-experiment study in which we
  combine previously explored data point size and contrast manipulations in
  scatterplots to influence viewer's estimates of correlation. We follow this 
  with an extended exploration into how the combination of two established 
  scatterplot manipulations can explain the relative contributions and mechanisms 
  of each, in ways that the manipulations in isolation cannot. We make the case
  for more extensive testing of these manipulations such that they can be tuned 
  to fully address a long-standing correlation underestimation bias by viewers 
  of scatterplots in the literature. Simultaneously we present a rigorous 
  experimental framework for fast, inexpensive, and empirical visualization design. 

---
```{r}
#| label: setup

set.seed(1234) # seed for all random number generation

# Loading packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(EMAtools)
library(geomtextpath)
# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("size_and_contrast_new/") }
```

```{r}
#| label: load-data

# load in data file
# Need higher guess_max so that read_csv() guesses column types correctly 

additive_anon <- read_csv("data/additive_data.csv", guess_max = 18001)
# tuning_anon <- read_csv("data/tuning_data.csv")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels, pattern = "vis_threshold_plots/", replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer, pattern = "_VT.png", replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct", "VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*0.5625,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch", "participant", "res_width")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later
# and allows us to use wrangle function for both experiments

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item", "contrast", "size"), sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    contrast == "S" & size == "S" ~ "A",
    contrast == "I" & size == "I" ~ "B",
    contrast == "S" & size == "I" ~ "X",
    contrast == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "contrast",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session",
                  "trials.thisN",
                  "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "contrast", "size", "condition_abs")), as_factor)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(additive_anon)

# set deviation coding for experimental model

contrasts(size_and_contrast_exp_tidy$size) <- matrix(c(.5, -.5))
contrasts(size_and_contrast_exp_tidy$contrast) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(additive_anon)

# extract age data

age <- distinct(size_and_contrast_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

  sum(is.na(size_and_contrast_exp_tidy$age_textbox.text))
  
# extract gender data

gender <- distinct(size_and_contrast_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(size_and_contrast_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract visual threshold data

VT <- size_and_contrast_exp_tidy %>%
  summarise(mean_VT_no_correct = mean(VT_no_correct),
            sd_VT_no_correct = sd(VT_no_correct),
            mean_VT_perc_correct = mean(VT_perc_correct),
            sd_VT_perc_correct = sd(VT_perc_correct))

# extract dot pitch data

dot_pitch <- size_and_contrast_exp_tidy %>%
  summarise(mean_dp = mean(dot_pitch),
            sd_dp = sd(dot_pitch))
```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# termS removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size * contrast)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
  
}
```

```{r}
#| label: effect-size-function

# function to calculate effects sizes in Cohen's d from models

get_effects_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r}
#| label: dot-plot-function

# function to create dot plots showing average correlation estimation errors and 95% CIs

dot_plot_function <- function(df, xlabel) {

data <- df %>%
  group_by(condition_abs) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(condition_abs)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1], # lower confidence interval
    hci = t.test(difference, conf.level = 0.95)$conf.int[2], # upper confidence interval
  )

  data %>%
    mutate(condition_abs = fct_reorder(condition_abs, mean)) %>%
    ggplot(aes(x = condition_abs, y = mean)) +
    geom_point(stat = "identity", size = 1) +
    geom_errorbar(aes(ymin=lci, ymax=hci), colour="black", width=0.01, linewidth =0.5) +
    theme_ggdist() +
    labs(x = xlabel,
         y = "Mean Error") +
    theme(axis.text = element_text(size = 13),
          axis.title = element_text(size = 16))
}
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 6.5),
        axis.title = element_text(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

```{r}
#| label: labelling-function

# creates labels vector for use with plotting functions

labels_size_contrast <- c(A = "Standard Orientation Size\nStandard Orientation Contrast",
                          B = "Inverted Orientation Size\nInverted Orientation Contrast",
                          X = "Standard Orientation Size\nInverted Orientation Contrast",
                          Y = "Inverted Orientation Size\nStandard Orientation Contrast")

labels_all_exp <- c(additive_manipulation = "Size and Contrast Manipulated",
                    contrast_manipulated = "Contrast Manipulated",
                    size_manipulated = "Size Manipulated",
                    standard_plot = "No Manipulation Present")
```

```{r}
#| label: make-sig-table-lm

# function to create table with fixed effects and interactions for LMM

make_sig_table <- function (model) {
  
  # buildmer class models need reassigned to Lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  table_df <- as.data.frame(summary(model)[10]) %>%
    rename("Estimate" = "coefficients.Estimate",
           "Standard Error" = "coefficients.Std..Error",
           "df" = "coefficients.df",
           "t-value" = "coefficients.t.value",
           "p" = "coefficients.Pr...t..") %>%
    mutate(p = scales::pvalue(p))
  
  # tidy up row names
  
  rownames(table_df) <- c("(Intercept)", "Size Decay", "Contrast Decay", "Size Decay x Contrast Decay")
  
  table <- kable(table_df, booktabs = TRUE, digits = c(2,3,2,2,2))
  
  return(table)
}
```

# Introduction

- why study scatterplots?
- correlation perception

We have previously demonstrated the potential for changes in point size and
point contrast to bias peoples' estimates of correlation in scatterplots. Lowering the
size and contrast of scatterplot points as they move further from the regression
line partially corrects for the underestimation bias described above. We have
hypothesized in our previous work that what is driving the changes in correlation estimation
that we have seen is an increase in spatial uncertainty at the edges of
the scatterplot relative to the center, which itself drives a reduction in the 
perceived width of the probability distribution said scatterplot represents. As part
of our ongoing effort to tune scatterplots for more accurate correlation perception,
our next step is to combine our point size and contrast manipulations. In the present
study we hypothesize that; an increased reduction in correlation estimation error will be
observed when standard orientation non-linear functions are used; the use of 
congruent inverted orientation conditions will produce 
the least accurate estimates of correlation; and that owing to the greater strength
of the size channel observed in @strain_2023b, there will be a significant difference in correlation
estimates between the two incongruent orientation conditions.

# Related Work {#sec-related-work}

## Testing Correlation Perception {#sec-testing-corr-percept}

## Drivers of Correlation Perception {#sec-drivers}

## Transparency and Contrast {#sec-transparency-and-contrast}

Changing the contrast of scatterplot points is standard practice to deal with
issues of overplotting or clutter (@matejka_2015; @bertini_2004); scatterplots
with very large numbers of points, especially with high degrees of overlap, suffer
from low individual-point visibility caused by high point density. Lowering
the contrast of all points addresses this, and makes data trends and distributions
easier to see and interpret. Previous work has found that lowering the contrast
of *all* scatterplot points relative to the background can increase the level of
underestimation error relative to full contrast, and that lowering point
contrast *as a function of distance from the regression line* is able to bias
correlation estimates upwards to partially correct for the underestimation 
bias (@strain_2023). The balance of evidence points towards an uncertainty-based
mechanism for these effects. Lower contrast can increase error in positional 
judgements (@wehrhahn_1990), can result in greater uncertainty in speed perception
(@champion_2017)


- formalising contrast
- ggplot etc
- use of contrast floor

- rehash contrast paper main points about contrast/alpha
- explain luminance, alpha, etc
- clarify use of alpha = 0.2 floor

## Point Size {#sec-point-size}

- interplay between size and spatial uncertainty
- hype up how strong the size effect was last time

# Methodology {#sec-methods}

## Crowdsourcing

- issues with crowdsourcing
- how those issues were solved
- why did we choose to crowdsource

Discussions about the transparency, contrast, and luminance is inherently difficult
within the context of online, crowdsourced experimental work. While the ease, low-cost,
and resilience to different viewing contexts afforded to us by such work is 
advantageous, we must also consider the impact on our ability to precisely 
describe the experimental stimuli.

## Open Research {#sec-open-research}

This study was conducted according to the principles of open and reproducible
research. All data and analysis code are available at (repository link removed for anon).
This repository contains instructions for building a docker image to fully reproduce the computational
environment used. This allows for full replications of stimuli, analysis, and the paper
itself. Ethical approval for both experiments was granted by (removed for anon). Hypotheses
and analysis plans were pre-registered with the OSF (links removed for anon).

## Scatterplot Generation {#sec-scatter-gen}

The data used to generate the scatterplots were identical to that used in
previous work [@strain_2023; @strain_2023b]. 45 scatterplot datasets were generated corresponding
to 45 *r* values uniformly distributed between 0.2 and 0.99, as there is evidence that
very little correlation is perceived below *r* = 0.2 [@strahan_1978; @bobko_1979; @cleveland_1982].
Using so many values for *r* allows us to paint a broader picture of people's perceptions
than work using fewer values. Scatterplot points were generated based on
bivariate normal distributions with standard deviations of 1 in each direction. Each
scatterplot had a 1:1 aspect ratio, was generated as a 1000*1000 pixel .png image,
and was scaled up or down according to a participant's monitor such that they
always occupied the same visual proportion. We used equation 1 to map residuals 
to size and contrast values.

\begin{equation}
  point_{size/contrast} = 1 - b^{residual}
\end{equation}

0.25 was chosen as the value of *b*. This is both due to its previous usage in studies
that the present work builds upon, and its production of a curve approximating the
inverse around the identity line of the underestimation curve reported in previous
work [@rensink_2017; strain_2023; strain_2023b]. We also acknowledge that there
may be other, more suitable values of *b*, however extensively testing these is
outside the scope of the present work. We used 2x2 combinations of this equation
applied to point size and contrast in standard and inverted orientation forms. 
Scatterplot examples can be seen in @fig-examples.

## Modelling {#sec-gen-modelling}

We use linear mixed effects models to model the relationships
between the combination of size and contrast decay
conditions and participants' errors in correlation estimates. Models such as these
allow us to compare differences in our IVs across the full range of participant 
responses, as opposed to relying purely on aggregate data, as in ANOVA. These models also afford us 
the ability to include random effects for participants and items. As per our pre-registrations
we preferred maximal models, including random intercepts and slopes for participants
and items. The structures of these models was identified using the **buildmer** package
in R (version xx, @voeten_buildmer). This package takes a maximal random effects
structure and then identifies the most complex model that converges, dropping
terms that fail to explain a significant amount of variance.

## Point Visibility Testing {#sec-VT}

It is key that our manipulations do not remove data from the scatterplot. We therefore
included point visibility testing to ensure this. Participants
viewed six scatterplots that were made up of a certain number of points. These points
were of the same size and contrast as the smallest and lowest contrast points used
in the experimental items. Participants were asked to enter in a textbox how many points
were present. Participants scored an average of
`r printnum(VT$mean_VT_perc_correct)`% ($SD$ = `r printnum(VT$sd_VT_perc_correct)`).
Despite our use of the contrast floor detailed in @sec-transparency-and-contrast,
it is clear that some of our small, low contrast points were not reliably visible, most likely
due to low contrast between the point and background, as previous work (strain_2023b) 
found point visibility invariant to size. We suggest this is due to differences in
monitors between participants. In reality this contrast floor would need to be
calibrated on a per-monitor basis. @fig-VT-hist shows distributions of participants'
performances on the visual threshold tests. We also include performance on the point
visibility test as a fixed effect in @sec-add-analyses.

```{r}
#| label: fig-VT-hist
#| include: true
#| fig-asp: 0.5
#| fig-cap: Histograms of visual threshold testing performance for experiment 1 (L) and experiment 2 (R).
#| out-width: "50%"
#| fig-align: "left"

# histograms of visual threshold testing performance for each experiment


ggplot(size_and_contrast_exp_tidy, aes(x = VT_no_correct)) +
  geom_histogram(binwidth = 1,colour = "grey", fill = "darkgrey") +
  theme_ggdist() +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 9)) +
  labs(x = "Number of Visual Threshold Tests Correct (out of 6)",
       y = "Count")
```

## Dot Pitch {#sec-dot-pitch}

We employed a method for obtaining the dot pitch of participants' monitors (@screenscale).
Combining this with monitor resolution information allows us to calculate the physical on-screen
size of scatterplot points. Participants were asked to hold a standard size credit/
debit/ID card (ISO/IEC 7810 ID-1) up to their screen and resize an on-screen card
until the two matched. We assumed a widescreen 16:9 aspect ratio and calculated
dot pitch based on these measurements. Mean dot pitch was 
`r printnum(dot_pitch$mean_dp, digits = 2)` ($SD$ = `r printnum(dot_pitch$sd_dp, digits = 2)`).
We include analyses with dot pitch as a fixed effect in @sec-add-analyses.

## Procedure {#sec-gen-procedure}

Both experiments were built using PsychoPy (@pierce_2019) and hosted on Pavlovia.org.
Participants were only permitted to complete the experiments on a desktop or laptop
computer. Each participant was first shown the participant information sheet and provided
consent through key presses in response to consent statements. They were asked to
provide their age in a free text box, followed by their gender identity. Participants
completed the 5-item Subjective Graph Literacy test (@garcia_2016), followed by the
visual threshold task described in @sec-VT and the screen scale task described in 
@sec-dot-pitch. Participants were given instructions, and were then shown examples
of scatterplots with correlations of *r* = 0.2, 0.5, 0.8, and 0.95, as piloting of
a previous experiment indicated some of the lay population may be unfamiliar
with the visual character of scatterplots. @sec-results contains further
discussion of the potential training effects of this. Two practice trials were
given before the experiment began. Participants worked through a randomly
presented series of 180 experimental trials (90 in experiment 2?) and were asked to
use a slider to estimate correlation to 2 decimal places. Visual masks preceded
each scatterplot. Interspersed were 6 attention check trials which explicitly
asked participants to ignore the scatterplot and set the slider to 0 or 1.

## Participants {#sec-participants}

150 participants were recruited using the Prolific.co platform. Normal to 
corrected-to-normal vision and English fluency were required for participation. To
ensure high quality data, and in accordance with guidelines published in @peer_2021,
participants were required to have successfully completed a minimum of 100 studies
on Prolific. In addition, participants who had completed any of our previous studies
into correlation estimation in scatterplots [@strain_2023; @strain_2023b, and a
previous pre-study] were prevented from participating.

Data were collected from 158 participants. 8 failed more than 2 our of 6 attention
check questions, and, as per pre-registration stipulations, were rejected from the
study. Data from the remaining 150 participants were included in the full analysis
(`r printnum(gender$M, digits = 1)`% male, `r printnum(gender$F, digits = 1)`% female,
and `r printnum(gender$NB, digits = 1)`% non-binary).
Participants mean age was `r printnum(age$mean, digits = 1)` (*SD* = `r printnum(age$sd, digits = 1)`).
Participants' mean graph literacy score was `r printnum(literacy$mean, digits = 1)`
(*SD* = `r printnum(literacy$sd, digits = 1)`). The average
time taken to complete the experiment was 37 minutes (*SD* = 12.3).

## Design {#sec-design}

We used a fully repeated-measures 2*2 factorial design in experiment 1. Each participant
saw each combination of size and contrast decay condition plots for a total of 180
experimental items. Participants viewed these experimental items, along with 6
attention check items, in a fully randomized order. Examples of experimental
items can be seen in @fig-examples.

```{r}
#| label: fig-examples
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"

example_plots <- function () {
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
  mutate(slope_inverted_floored = pmax(0.1,(1+(0.25)^my_residuals)-1)) 
  
plot_example_function <- function (data, size, contrast, title) {
  
set.seed(1234)
  
  ggplot(data, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(size + 0.2), alpha = contrast), shape = 16)  +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 15)) +
  labs(title = t)

}  

plots <- ggarrange(plot_example_function(slopes, (1-slopes$slope_0.25), (1-slopes$slope_0.25), "A"),
                   plot_example_function(slopes, (1-slopes$slope_inverted), (1-slopes$slope_inverted), "B"),
                   plot_example_function(slopes, (1-slopes$slope_inverted), (1-slopes$slope_0.25), "X"),
                   plot_example_function(slopes, (1-slopes$slope_0.25),(1-slopes$slope_inverted), "Y"))

return(plots)

}
```



# Results {#sec-results}

Our first two hypotheses were fully supported in this experiment. The combination of 
non-linear size and contrast decay functions produced the most accurate estimates of
correlation, although this also resulted in a large correlation overestimation for many
values of *r* (see @fig-diff-error-bars-plot). Our second hypothesis was also 
supported; the combination of inverted size and inverted contrast decay conditions
produced the least accurate estimates of correlation. We found no support for our third
hypothesis; there was no significant difference in correlation estimates for
non-linear size/inverted contrast decay plots and inverted size/non-linear contrast
decay plots (Z = -2.26, *p* = .11), however we did find a significant interaction effect
that provides evidence that the size decay function was stronger with regards
to biasing people's estimates of correlation.

```{r}
#| label: model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# r estimation error modelling

model <- buildmer(difference ~ size * contrast +
                       (1 + size * contrast | participant) +
                       (1 + size * contrast | item),
                     data = size_and_contrast_exp_tidy)
```

```{r}
#| label: assign-slot

model <- model@model
```

```{r}
#| label: model-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# buildcomparison model with both fixed effects terms removed

model_cmpr <- comparison(model)
```

```{r}
#| label: anova

# runs ANOVA between experimental and comparison models

anova_results(model, model_cmpr)
```

```{r}
#| label: fig-dot-plot
#| include: false
#| fig-cap: Mean r estimation for each combination of conditions. 95% confidence intervals are shown as error bars.
#| fig-asp: 0.6
#| out-width: 50%
#| fig-align: "left"

# dot plot of mean errors and 95% CIs for each combination of size and contrast decay conditions
# currently not used in - EMM plot used in place

dot_plot_function(size_and_contrast_exp_tidy, "Size Decay : Contrast Decay") +
    scale_x_discrete(labels = labels_size_contrast) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 9))
```

```{r}
#| label: tbl-sig
#| include: true
#| tbl-cap: Significances of fixed effects and interaction for experiment 1.

make_sig_table(model)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`).
Deviation coding was used for each of the experimental factors. We used the **buildmer** and
**lme4** packages to build a linear mixed effects model where the difference
between objective and rated r value was predicted by the size and contrast decay
conditions used. A likelihood ratio test revealed that the model including point size
and contrast decay conditions as fixed effects explained significantly more variance
than the null ($\chi^2$(`r in_paren(model.df)`) = 
`r printnum(model.Chisq)`, *p* `r printp(model.p, add_equals = TRUE)`). There
were significant fixed effects of size decay and contrast decay conditions, as well
as a significant interaction between the two. @tbl-sig shows a summary
of the model statistics. The experimental model has random intercepts for items
and participants, and a random slope for the size decay factor with regards to participants.

```{r}
#| label: fig-emm-plot
#| include: true
#| fig-asp: 0.7
#| out-width: "50%"
#| fig-align: "left"

plot(emmeans(model, pairwise ~ size * contrast)[[1]]) +
  labs(x = "Estimated Marginal Mean",
       y = "Pairs") +
  theme_ggdist() +
  theme(axis.text = element_text(size = 13),
        axis.title = element_text(size = 16)) +
  scale_y_discrete(labels = c("Inv Size\nNL Contrast",
                              "NL Size\nNL Contrast",
                              "Inv Size\nInv Contrast",
                              "NL Size\nInv Contrast ")) +
  coord_flip()
```

```{r}
#| label: fig-interaction-plot
#| include: true
#| fig-cap: Interaction plot showing the moderating effect of encoding channel on decay orientation.
#| fig-asp: 0.7
#| out-width: 50%
#| fig-align: "left"

emm <- emmeans(model, ~ size * contrast)

emm %>%
  as_tibble() %>%
  mutate("contrast" = recode(contrast,
                             "I" = "Inverted Decay",
                             "N" = "Non-linear Decay")) %>%
  ggplot() +
    aes(x = size, y = emmean, colour = contrast) +
    geom_line(aes(group = contrast), size = 1) +
    geom_point(size = 3) +
    theme_ggdist() +
    ylim(-0.1,0.175) +
    scale_x_discrete(labels = c("Non-Linear Decay", "Inverted Decay")) +
    labs(x = "Size Decay Condition",
         y = "Estimated Marginal Mean",
         colour = "Contrast\nDecay\nCondition") +
  theme(legend.position = c(0.75,0.3),
        axis.text = element_text(size = 13),
        axis.title = element_text(size = 16))
```

```{r}
#| label: tbl-contrasts
#| include: true
#| tbl-cap: Pairwise comparisons for experiment 1. Our interaction is driven by the greater strength of the size channel, whether non-linear or inverted decay functions were used. Note the lack of significance for NL size/Inv contrast against Inv size/NL contrast comparison.

table_df <- contrasts_extract(model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  mutate('Contrast' = recode(Contrast,
                             "N I - I I" = "Non-linear Size x Inverted Contrast <-> Inverted Size x Inverted Contrast", #
                             "N I - N N" = "Non-linear Size x Inverted Contrast <-> Non-Linear Size x Non-linear Contrast",
                             "N I - I N" = "Non-linear Size x Inverted Contrast <-> Inverted Size x Non-linear Contrast",
                             "I I - N N" = "Inverted Size x Inverted Contrast <-> Non-linear Size x Non-linear Contrast",
                             "I I - I N" = "Inverted Size x Inverted Contrast <-> Inverted Size x Non-linear Contrast",
                             "N N - I N" = "Non-linear Size x Non-linear Contrast <-> Inverted Size x Non-linear Contrast"))

kable(table_df, booktabs = TRUE, digits = c(0,2,3))
```

The **emmeans** (cite) package was used to run pairwise comparisons. @fig-emm-plot shows
estimated marginal means for each combination of factors, and @fig-interaction-plot 
shows the form the interaction takes. The interaction is driven by there being a
greater difference in estimates between NL and inverted size decay conditions when
the contrast decay condition is held as non-linear,
as opposed to when it is inverted. This is parsimonious with our previous work 
demonstrating the greater capacity of the size encoding channel to bias
participants' estimates of *r* (@strain_2023b), and our findings that the inverted
channel is generally weaker at biasing correlation in the opposite direction 
(@strain_2023; @strain_2023b). @tbl-contrasts shows statistics for pairwise
comparisons.

```{r}
#| label: fig-diff-error-bars-plot
#| include: true
#| out-width: 100%
#| fig-cap: Plots showing how participants' correlation estimation errors change as a function of the *r* value for each combination of size and contrast decay factors.
#| fig-asp: 1

plot_error_bars_function(size_and_contrast_exp_tidy %>%
                        mutate(condition_abs = fct_relevel(condition_abs,
                                                          c("A", "B", "X", "Y"))),
                        "difference",
                        labels_size_contrast) +
  geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: additional-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

lit_model <- lmer(add.terms(formula(model), "literacy"),
                  data = size_and_contrast_exp_tidy)

anova_results(lit_model, model)

VT_model <- lmer(add.terms(formula(model), "VT_no_correct"),
                  data = size_and_contrast_exp_tidy)

anova_results(VT_model, model)

dot_pitch_model <- lmer(add.terms(formula(model), "dot_pitch"),
                        data = size_and_contrast_exp_tidy)

anova_results(dot_pitch_model, model)
```

## Additional Analyses {#sec-add-analyses}

We find no effects of graph literacy ($\chi^2$(`r in_paren(lit_model.df)`) = 
`r printnum(lit_model.Chisq)`, *p* `r printp(lit_model.p, add_equals = TRUE)`) or
performance on the visual threshold task ($\chi^2$(`r in_paren(VT_model.df)`) = 
`r printnum(VT_model.Chisq)`, *p* `r printp(VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(dot_pitch_model.df)`) = `r printnum(dot_pitch_model.Chisq)`,
*p* `r printp(dot_pitch_model.p, add_equals = TRUE)`) on participants' errors in correlation estimation.

# Discussion {#sec-discussion}

Our findings here provide further confirmatory evidence of what has been found
previously with regards to the effects of point size and contrast manipulations
on correlation estimation in scatterplots. Namely, that while both manipulations
have a significant effect, the effect of changing point sizes is stronger, and that
while we can influence correlation estimates in either direction, standard
orientation manipulations are more powerful than inverted ones. The experiment
described above additionally contributes evidence of a congruency effect; effects
are most strong when there is orientation congruency between size and contrast
manipulations. 

The lack of support for our third hypothesis, that there would be a
difference in correlation estimates between incongruent conditions, was surprising
given the greater strength of the size channel relative to contrast. We take
this as evidence that the combination of size and contrast manipulations is not
additive, as we had initially suspected, which explains the interaction we see here.
Integrating the data collected in the present study with previously collected
data investigating the effects of the size and contrast manipulations in isolation
allows us demonstrate this and to partial out the effects each manipulation.

```{r}
#| label: fig-error-bars-all-exp
#| include: true
#| out-width: "100%"
#| fig-cap: From left to right, plotting *r* estimation error against the objective *r* value for the standard orientation condition in the present study, for standard orientation size and contrast manipulations in previous work, and for normal scatterplots averaged over identical conditions in previous work. Error bars have been left off this plot to make interpretation more simple.

# dataframe containing values from previous experiments and the current is included
# in the data folder 

all_exp_df <- read_csv("data/all_exp.csv")
  
  all_exp_df %>% 
    drop_na() %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    #geom_point(size = 0.2) + 
    #geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 6.5),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 4, labeller = labeller(factor = labels_all_exp)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: fig-power-plot
#| include: true
#| out-width: "100%"
#| fig-cap: additive_raw_pl = observed values for present study. standard_curve = no manipulation averaged across all experiments

# dataframe containing values from previous experiments and the current is included
# in the data folder 
curves <- read_csv("data/curves_df.csv") 

  curves %>% 
    drop_na() %>%
    select(c("contrast_power", "size_power", "additive_power", "standard_curve", "additive_raw_pl", "my_rs")) %>%
    pivot_longer(cols = c("additive_raw_pl",
                          "contrast_power",
                          "size_power",
                          "additive_power",
                          "standard_curve"), names_to = "factor", values_to = "power") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(power), mean = mean(power)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    #geom_point(size = 0.2) + 
    #geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 6.5),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 5) +
    labs(x = "Objective r",
         y = "Power") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) 
    #geom_hline(yintercept = 0, linetype = 2)


```

As can be seen in @fig-error-bars-all-exp, combining the size and contrast manipulations
results in an error curve of a mostly similar shape to that of the size manipulation.
The difference that comes from adding the contrast manipulation lies in the severity
of the effect and the orientation of the curve itself. Transforming the size underestimation
curve from @strain_2023 into the size and contrast underestimation curve we have
found in the present study gives us some insight into the effect of adding 
the two manipulations together.

```{r}
#| label: fig-transformation
#| include: true
#| out-width: "100%"
#| fig-asp: 0.333
#| fig-cap: Transformations required to derive the Combination Curve observed in the present experiment using the size and contrast curves from previous work. The dotted line represents the size curve (A), two times the size curve (B), and two times the size curve rotated by 6 degrees clockwise.



p1 <- curves %>%
  select(c("my_rs", "size_raw", "additive_raw_pl", "standard_curve")) %>%
  pivot_longer(cols = -c("my_rs", "size_raw"), names_to = "factor", values_to = "value") %>%
  ggplot(aes(x = my_rs, y = value, group = factor)) +
  theme_ggdist() +
  geom_abline(slope = 1, size = 0.2) +
  ylim(0.2,1) +
  xlim(0.2,1) +
  geom_smooth(aes(x = my_rs, y = size_raw), se = F, size = 0.4, colour = "black", linetype = "dotted") +
  geom_smooth(colour = "black", se = F, size = 0.4) +
  theme(legend.position = "none") +
  labs(x = "",
       y = "Estimate") +
  theme(plot.title = element_text(size = 7),
        axis.title = element_text(size = 8),
        axis.text = element_text(size = 6),
        legend.position = "none") +
  annotate(label = "Combination Curve\n(present experiment)", geom = "text", x = 0.38, y = 0.85, size = 1.75) +
  annotate(label = "No Manipulation", geom = "text", x = 0.8, y = 0.3, size = 1.75) +
  annotate("segment", x = 0.52, y = 0.78, xend = 0.55, yend = 0.7, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("segment", x = 0.8, y = 0.325, xend = 0.65, yend = 0.41, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("label", x = 0.25, y = 0.975, label = "A", size = 2)

p2 <- curves %>%
  select(c("my_rs", "two_size_added", "additive_raw_pl", "standard_curve")) %>%
  pivot_longer(cols = -c("my_rs", "two_size_added"), names_to = "factor", values_to = "value") %>%
  ggplot(aes(x = my_rs, y = value, group = factor)) +
  theme_ggdist() +
  geom_abline(slope = 1, size = 0.2) +
  ylim(0.2,1) +
  xlim(0.2,1) +
  geom_smooth(aes(x = my_rs, y = two_size_added), se = F, size = 0.4, colour = "black", linetype = "dotted") +
  geom_smooth(colour = "black", se = F, size = 0.4) +
  theme(legend.position = "none") +
  labs(x = "Objective r",
       y = "") +
  theme(plot.title = element_text(size = 7),
        axis.title = element_text(size = 8),
        axis.text = element_text(size = 6),
        legend.position = "none") +
  annotate(label = "Combination Curve\n(present experiment)", geom = "text", x = 0.38, y = 0.85, size = 1.75) +
  annotate(label = "No Manipulation", geom = "text", x = 0.8, y = 0.3, size = 1.75) +
  annotate("segment", x = 0.52, y = 0.78, xend = 0.55, yend = 0.7, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("segment", x = 0.8, y = 0.325, xend = 0.65, yend = 0.41, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("label", x = 0.25, y = 0.975, label = "B", size = 2)

p3 <- curves %>%
  select(c("my_rs", "rotated_curve", "additive_raw_pl", "standard_curve")) %>%
  pivot_longer(cols = -c("my_rs", "rotated_curve"), names_to = "factor", values_to = "value") %>%
  ggplot(aes(x = my_rs, y = value, group = factor)) +
  theme_ggdist() +
  geom_abline(slope = 1, size = 0.2) +
  ylim(0.2,1) +
  xlim(0.2,1) +
  geom_smooth(aes(x = my_rs, y = rotated_curve), se = F, size = 0.4, colour = "black", linetype = "dotted") +
  geom_smooth(colour = "black", se = F, size = 0.4) +
  labs(x = "",
       y = "") +
  theme(plot.title = element_text(size = 7),
        axis.title = element_text(size = 8),
        axis.text = element_text(size = 6),
        legend.position = "none") +
  annotate(label = "Combination Curve\n(present experiment)", geom = "text", x = 0.38, y = 0.85, size = 1.75) +
  annotate(label = "No Manipulation", geom = "text", x = 0.8, y = 0.3, size = 1.75) +
  annotate("segment", x = 0.52, y = 0.78, xend = 0.55, yend = 0.7, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("segment", x = 0.8, y = 0.325, xend = 0.65, yend = 0.41, arrow = arrow(length = unit(0.1, "cm")), linewidth = 0.25) +
  annotate("label", x = 0.25, y = 0.975, label = "C", size = 2)



ggarrange(p1,p2,p3, ncol = 3)
```

@fig-transformation illustrates the transformations we must do to replicate the 
results we have found in the present study using results from previous work 
(@strain_2023; @strain_2023b). Given the similarity between the shape of the
combination curve found in the present study and that of the size only manipulation
found in @strain_2023b, we use this as a starting point. Adding the contrast manipulation
has the effect of doubling the power of the size curve, followed by a change in
orientation that can be approximated by a 6\textdegree clockwise rotation. With the
knowledge that the addition of the contrast manipulation can be thought of
as doubling and rotating the size factor, and having worked out *t*, a translation
function to approximate the $y = x$ optimum estimation line, we are able to generate
the following set of equations. S = size, C = contrast, O = observed (present study),
r = rotation function, t = translation function.

\begin{equation}
  S + C = O
  r(S + C) = O
  C = S
  r(2S) = O
  tr(2S) = tO
\end{equation}


# References {-}


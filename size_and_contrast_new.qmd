---
format: acm-pdf

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
keep-tex: true

bibliography: size-contrast-new.bib

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

title: Further Investigating the Effects of Point Size and Contrast on Correlation Perception in Scatterplots

# if short-title is defined, then it's used
short-title: Size, Contrast, and Scatterplots

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: manuscript, review, anonymous, screen  

  # acm preamble information
  copyright-year: 2018
  acm-year: 2018
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation emai
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
     <concept>
      <concept_id>10010520.10010553.10010562</concept_id>
      <concept_desc>Computer systems organization~Embedded systems</concept_desc>
      <concept_significance>500</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010575.10010755</concept_id>
      <concept_desc>Computer systems organization~Redundancy</concept_desc>
      <concept_significance>300</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010553.10010554</concept_id>
      <concept_desc>Computer systems organization~Robotics</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
     <concept>
      <concept_id>10003033.10003083.10003095</concept_id>
      <concept_desc>Networks~Network reliability</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
    </ccs2012>
    \end{CCSXML}
    
    \ccsdesc[500]{Computer systems organization~Embedded systems}
    \ccsdesc[300]{Computer systems organization~Redundancy}
    \ccsdesc{Computer systems organization~Robotics}
    \ccsdesc[100]{Networks~Network reliability}

  keywords:
    - correlation
    - scatterplot
    - perception
    - crowdsourced

abstract: |
  Changing the size and contrast of points on scatterplots can be used to systematically
  alter viewers' perceptions of correlation. Point size and contrast adjustments operate on correlation perception
  through different perceptual mechanisms, and combining them produces effects
  that are not additive in nature. We present a fully reproducible 
  study in which we combine previously established techniques for
  influencing correlation perception to show that there are few limits to the extent to which
  we can use visual features to change viewers' perceptions of data visualizations. We follow
  this with an extended discussion into how our findings corroborate previous work on
  correlation perception in scatterplots and can be used to correct for a historic perceptual bias.
  
---
```{r}
#| label: setup

set.seed(1234) # seed for all random number generation

# Loading packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(EMAtools)
library(ggtext)
library(geomtextpath)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("size_and_contrast_new_cache/") }
```

```{r}
#| label: load-data

# load in data file
# Need higher guess_max so that read_csv() guesses column types correctly 

additive_anon <- read_csv("data/additive_data.csv", guess_max = 18001)
# tuning_anon <- read_csv("data/tuning_data.csv")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels",
             "participant",
             "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct",
           "VT_no_correct",
           "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*0.5625,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch",
                  "participant",
                  "res_width")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later
# and allows us to use wrangle function for both experiments

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-I")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item",
                     "contrast",
                     "size"),
           sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    contrast == "S" & size == "S" ~ "A",
    contrast == "I" & size == "I" ~ "B",
    contrast == "S" & size == "I" ~ "X",
    contrast == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
            "item",
            "size",
            "contrast",
            "slider.response",
            "my_rs",
            "total_residuals",
            "unique_item_no",
            "session",
            "trials.thisN",
            "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "contrast", "size", "condition_abs")), as_factor)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(additive_anon)

# set deviation coding for experimental model

contrasts(size_and_contrast_exp_tidy$size) <- matrix(c(.5, -.5))
contrasts(size_and_contrast_exp_tidy$contrast) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(additive_anon)

# extract age data

age <- distinct(size_and_contrast_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

  sum(is.na(size_and_contrast_exp_tidy$age_textbox.text))
  
# extract gender data

gender <- distinct(size_and_contrast_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(size_and_contrast_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract visual threshold data

VT <- size_and_contrast_exp_tidy %>%
  summarise(mean_VT_no_correct = mean(VT_no_correct),
            sd_VT_no_correct = sd(VT_no_correct),
            mean_VT_perc_correct = mean(VT_perc_correct),
            sd_VT_perc_correct = sd(VT_perc_correct))

# extract dot pitch data

dot_pitch <- size_and_contrast_exp_tidy %>%
  summarise(mean_dp = mean(dot_pitch),
            sd_dp = sd(dot_pitch))
```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# termS removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size * contrast)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
  
}
```

```{r}
#| label: effect-size-function

# function to calculate effects sizes in Cohen's d from models

get_effects_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r}
#| label: dot-plot-function

# function to create dot plots showing average correlation estimation 
# errors and 95% CIs

dot_plot_function <- function(df, xlabel) {

data <- df %>%
  group_by(condition_abs) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(condition_abs)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1], # lower confidence interval
    hci = t.test(difference, conf.level = 0.95)$conf.int[2], # upper confidence interval
  )

  data %>%
    mutate(condition_abs = fct_reorder(condition_abs, mean)) %>%
    ggplot(aes(x = condition_abs, y = mean)) +
    geom_point(stat = "identity", size = 1) +
    geom_errorbar(aes(ymin=lci, ymax=hci), colour="black", width=0.01, linewidth=0.5) +
    theme_ggdist() +
    labs(x = xlabel,
         y = "Mean Error") +
    theme(axis.text = element_text(size = 14),
          axis.title = element_text(size = 16))
}
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),
                width = 0.01,
                size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6,
                                  margin = margin(1,0,1,0, "mm")),
        aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

```{r}
#| label: labelling-function

# creates labels vector for use with plotting functions

labels_size_contrast <- c(A = "Standard Orientation Size\nStandard Orientation Contrast",
                          B = "Inverted Orientation Size\nInverted Orientation Contrast",
                          X = "Standard Orientation Size\nInverted Orientation Contrast",
                          Y = "Inverted Orientation Size\nStandard Orientation Contrast")

labels_all_exp <- c(additive_manipulation = "Size and Contrast Manipulated",
                    contrast_manipulated = "Contrast Manipulated",
                    size_manipulated = "Size Manipulated",
                    standard_plot = "No Manipulation Present")

labels_power <- c(size_power = "Size\nPower",
                  contrast_power = "Contrast\nPower",
                  additive_power = "Size and Contrast\nCombined Power")
```

```{r}
#| label: make-sig-table-lm

# function to create table with fixed effects and interactions for LMM

make_sig_table <- function (model) {
  
  # buildmer class models need reassigned to Lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  table_df <- as.data.frame(summary(model)[10]) %>%
    rename("Estimate" = "coefficients.Estimate",
           "Standard Error" = "coefficients.Std..Error",
           "df" = "coefficients.df",
           "t-value" = "coefficients.t.value",
           "p" = "coefficients.Pr...t..") %>%
    mutate(p = scales::pvalue(p))
  
  # tidy up row names
  
  rownames(table_df) <- c("(Intercept)",
                          "Size Decay",
                          "Contrast Decay",
                          "Size Decay x Contrast Decay")
  
  table <- kable(table_df, booktabs = TRUE, digits = c(2,3,2,2,2))
  
  return(table)
}
```

```{r}
#| label: plot-examples-function
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
  mutate(slope_inverted_floored = pmax(0.1,(1+(0.25)^my_residuals)-1)) 
  
plot_example_function <- function (data, size, contrast, title) {
  
set.seed(1234)
  
  ggplot(data, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(size + 0.2),
                 alpha = contrast),
             shape = 16)  +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 13),
        axis.title = element_blank()) +
  labs(title = title)

} 
```

# Introduction

Scatterplots are common bivariate representations of data. They have been extensively
studied and are used for a variety of communicative tasks. They are most commonly
used to represent linear correlation, or the degree of linear relatedness between two variables,
but are also used to represent different groups (clustering), to aid in the detection of outliers,
to characterize distributions, and to visualize non-linear correlations.
@fig-tasks contains examples of scatterplots optimised for different tasks. There
is evidence that people generally interpret them in similar ways [@kay_2015], and
that they support the interpretation of correlation significantly better than other
data visualizations [@li_2010]. Rapid interpretation by viewers [@rensink_2014] along
with low levels of interindividual variance render scatterplots particularly suited
for experimental work; they provide important insights into perception and visualization design while
being simple to study. 

While our interpretations of scatterplots are generally similar, the accuracy of those
interpretations is generally low. Viewers systematically underestimate the correlation
displayed in positively correlated scatterplots. This holds true for direct 
estimation tasks [@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989;
@collyer_1990; @meyer_1992], and estimation via bisection tasks [@rensink_2017], and
is particularly pronounced between 0.2 < *r* < 0.6. The COVID-19 pandemic demonstrated
that even outside of professional/office environments, lay 
populations are now expected to be able to use and accurately interpret data
visualizations on a daily basis [@bbc_2022]. This expectation confers a responsibility on
the part of data visualization designers to design in such a way that people with
little statistical or graph training can be expected to correctly interpret
data visualizations. Doing this requires us to understand human perception and
perceptual phenomena, apply this understanding to data visualization design, and
test those designs in rigorous empirical work. We therefore present a fully-reproducible,
crowdsourced online experiment in which we systematically alter visual features
in scatterplots to correct for a historic bias. We combine two techniques for
correcting for correlation underestimation in scatterplots and show that the combined
effect is stronger than one would expect were they linearly additive. Through
this work we also present a framework for visualization design 
informed from the ground up by human perception.

```{r}
#| label: fig-tasks
#| include: true
#| out-width: "100%"
#| fig-asp: 0.25
#| fig-cap: Examples of scatterplots designed for different scatterplot-associated tasks. Both colour and point shape have been used to delineate different clusters in the cluster separation plot.

# set random seed for reproducibility

set.seed(123)

# create theme so that newly generated plots match the plot example function

tasks_theme <- function() {
               theme_classic() %+replace%
               theme(axis.text = element_blank(),
               plot.margin = unit(c(0,0,0,0), "cm"),
               legend.position = "none",
               plot.title = element_text(size = 7.5, hjust = 0, vjust = 1),
               axis.title = element_blank(),
               axis.line = element_line(linewidth = 0.25),
               axis.ticks = element_line(linewidth = 0.25),
               axis.ticks.length = unit(1.375, "pt"))
}

## Create correlation plot

corr_plot <- plot_example_function(slopes, 0.05, 0,  "Correlation\nPerception") + tasks_theme() + geom_point(aes(alpha = 1), size = 0.5)

## Create cluster separation plot

# cluster sep data

n <- 128

cluster_sep_data <- data.frame(
  x = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 8, sd = 1)),
  y = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 0, sd = 1)),
  cluster = factor(rep(1:3, each = n)))

# create plot

cluster_sep_plot <- ggplot(cluster_sep_data, aes(x = x, y = y, colour = cluster)) +
  geom_point(size = 0.6, aes(shape = cluster)) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Cluster\nSeparation")

## Outlier detection plot

# make data

n <- 127
x <- runif(n, 0, 100)
y <- 2 * x + 5 + rnorm(n, 0, 10) 
x <- c(x, 35) 
y <- c(y, 200) 

data <- data.frame(x, y)

# create plot

outlier_plot <- ggplot(data, aes(x, y)) +
  geom_point(size = 0.2) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Scatterplot with\nObvious Outlier") +
  ylim(0,300)

## Non-linear correlation plot

# make data
  
V1 <- seq(-10, 10, length.out = 128)

V2 <- 2 * V1^2 + rnorm(length(V1), mean = 0, sd = 10)

NL_data <- data.frame(V1, V2)

# make plot

NL_corr_plot <- plot_example_function(NL_data, 0.05, 0, "Non-linear\ncorrelation") + tasks_theme() + geom_point(aes(alpha = 1), size = 0.2)

# arrange the four task example plots

ggarrange(corr_plot, cluster_sep_plot, outlier_plot, NL_corr_plot, nrow = 1)
```


# Related Work {#sec-related-work}

## Testing Correlation Perception {#sec-testing-corr-percept}

The testing of linear correlation perception in scatterplots has a long and rich history, and has
explored a wide variety of plot types, tasks, and modelling methods. Work 
has had participants make discriminative judgements between scatterplots with different
correlations [@pollack_1960; @doherty_2007], finding that performance on such a task became better
as the objective *r* value increased. This performance on a discriminative judgement
task can also be modelled by deep neural networks [@yang_2023]. Extensive work throughout the 1970's to 1990's 
focused primarily on having participants produce a numerical estimate of correlation,
finding evidence for a systematic underestimation for positive *r* values besides
0 and 1. This underestimation was especially pronounced for 0.2 < *r* < 0.6
[@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989; @collyer_1990; @meyer_1992]
and is demonstrated using an equation relating objective to subjective correlation [@rensink_2017] in
@fig-underestimation-curve. More recent work has attempted to model participants' correlation estimation performance
by using a combination of bisection task, in which participants were asked to adjust a
plot until its correlation was halfway between two reference plots, and a staircase
task designed to produce Just Noticeable Differences between scatterplots such
that they are discriminable 75% of the time [@rensink_2010]. This work has
also been extended to incorporate Bayesian data analysis [@kay_2015]. The current experiment
takes similar techniques from previous work [@strain_2023; @strain_2023b] and 
combines them to further push the envelope of how systematically adjusting visual
features in scatterplots can radically alter people's perceptions of correlation.
For this reason we use the same direct estimation paradigm  to collect responses.
This paradigm allows for a large number of judgements to be collected,
and is simple enough that participants need little to no training.

## Drivers of Correlation Perception {#sec-drivers}

Evidence points towards correlation perception being driven by the shape of the
underlying probability distribution represented by scatterplot points, however it should
be noted that this is very much still an open question, and it may be the case that
there are different contributory perceptual mechanisms operating at different levels
based on task-specific differences such as viewing time and levels of graph-training.
Increasing the *x* and *y* scales on a scatterplot such that the size of the point
cloud decreases [@cleveland_1982] is associated with an increase in a viewer's
judgements of bivariate association, despite the objective *r* value remaining the same. It
was suggested in this case that viewers may have been using the area of the point
cloud to judge association. Later work found that the relationship between objective
and perceived *r* values could be described by a function that included the mean of the
geometric distances between the points and the regression line [@meyer_1997]. Investigations
of the idea that people use visual features to judge correlation provide
evidence that, among others features, the standard deviation of all perpendicular
distances from scatterplot points to the regression line was predictive of performance
on a correlation estimation task [@yang_2019]. Equations for both discrimination
and magnitude estimation of *r* in scatterplots include a quantity that is small
when *r* = 1 and increases as *r* approaches 0 [@rensink_2017]. This quantity is indifferent
to the type of visualization used, and is functionally similar to that found
in work mentioned above [@cleveland_1982; @meyer_1997; @yang_2019]. Regarding
scatterplots, this quantity represents the average distance between data points
and the regression line, and can be thought of as representing the width
of the underlying distribution. Findings from deep neural networks also support 
the idea that viewers are using an aspect of scatterplot shape to judge correlation,
or some measure of what has been termed *dot entropy* [@yang_2023], again considered a candidate
visual proxy for correlation judgements [@rensink_2017; @rensink_2022].

Recent work investigating the use of decay functions that change point size or contrast
in scatterplots as a function of residual distance provide further evidence for 
both point density and salience/perceptual weighting being drivers of correlation perception.
The use of an inverted contrast decay function [@strain_2023], such that point contrast
decreased as a point approached the regression line, resulted in significantly
lower and less accurate correlation estimates compared to data-identical plots
with the same overall shape, implying that the center of the scatterplot not
being **filled**  biased viewers' perceptions down. When point contrast
or size are reduced as a function of distance from the regression line 
[@strain_2023; @strain_2023b], viewers rate correlation as significantly higher
and are significantly more accurate, which supports a low-level data salience
account. It is more difficult to comment on higher level perceptual mechanisms,
and in any case this is not the intended contribution of the present work; we aim
to test the impact of systematically altering visual features on correlation perception, 
and to provide empirically-derived tools for visualization designers to design better
visualizations through the use of a simple, reproducible framework.

```{r}
#| label: fig-underestimation-curve
#| include: true
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"
#| fig-cap: Using a function relating objective to perceived *r* value [@rensink_2017] provides a visualization of the nature of correlation underestmiation reported in previous work. An identity line has been included to illustrate where viewers are most and least accurate.

my_rs <- seq(from = 0, to = 1, length.out = 100)

as_tibble(my_rs) %>%
  mutate(under_est = (log(1-0.88*my_rs)/log(1-0.88))) %>%
  ggplot() +
  geom_smooth(aes(x = my_rs, y = under_est), colour = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  ylim(0,1) +
  xlim(0,1) +
  theme_ggdist() +
  theme(axis.title.x = ggtext::element_markdown(size = 16),
        axis.title.y = ggtext::element_markdown(size = 16),
        axis.text = element_text(size = 14)) +
  labs(x = "Objective *r* value",
       y = "Subjective *r* value")

```

## Transparency and Contrast {#sec-transparency-and-contrast}

Changing the contrast of scatterplot points is standard practice to deal with
issues of overplotting or clutter [@matejka_2015; @bertini_2004]; scatterplots
with very large numbers of points, especially with high degrees of overlap, suffer
from low individual-point visibility caused by high point density. Lowering
the contrast of all points addresses this, and makes data trends and distributions
easier to see and interpret (see @fig-overplotting-examples).

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.75
#| out-width: "50%"
#| fig-align: "left"
#| fig-cap: Adjusting point contrast to address overplotting. Contrast between the points and the background is full (alpha = 1, L) or low (alpha = .1, R). The dataset used has 40,000 points.
 
set.seed(123)

data <- data.frame(x = c(rnorm(20000, mean = -1),
                         rnorm(20000, mean = 1)),
                   y = rnorm(40000))

ggplot(data, aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

```{r}
#| label: fig-alpha-examples
#| include: true
#| out-width: "50%"
#| fig-cap: Demonstrating the effects of different alpha values on point contrast.
#| fig-height: 2
#| fig-align: "left"

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)
ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 18, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11)) +
rm(x1,y1,df)
```

In the present study we use the **ggplot2** package [@hadley_gg2016] to create
our stimuli. This package uses an alpha parameter, or the level of linear
interpolation [@stone_2008] between foreground and background pixel values, to
set the contrast of points. As demonstrated in @fig-alpha-examples, an alpha value
of 0 or 1 results in no interpolation and renders either the background or foreground
pixel values respectively. Psychophysical definitions of perceived contrast are
often based on what is being presented (i.e gratings) or are modelled to take
into account phenomena of human vision (i.e visibility limits) [@zuffi_2007].
Our crowdsourced methodology gives us no control the exact luminances
of our stimuli, only over the *relative* differences in luminance between scatterplot
points and backgrounds. For this reason we do not report absolute luminance nor
make any attempt to adopt a formal definition of contrast; instead we simply
report the alpha value used. Given that our work aims to improve correlation perception
without removing data from the scatterplot, we also incorporate point visibility testing 
(see @sec-VT). Informal point visibility piloting suggested that our smallest,
lowest contrast points had very low visibility. We therefore implemented an alpha = 0.2 floor
for these points, which we felt conferred a sufficient level of point visibility.

Previous work has found that lowering the contrast of *all* 
scatterplot points relative to the background can increase the level of
underestimation error relative to full contrast, and that lowering point
contrast *as a function of distance from the regression line* is able to bias
correlation estimates upwards to partially correct for the underestimation 
bias [@strain_2023]. Evidence suggests point salience/perceptual weighting and
spatial uncertainty as drivers for this effect. Lower stimulus contrast is
associated with lower salience [@healey_2011], can bias judgements of mean point
position [@hong_2021] and increase error in positional judgements [@wehrhahn_1990],
and can result in greater uncertainty in speed perception [@champion_2017]. Due 
to effects of both salience/perceptual weighting and spatial uncertainty operating
in the same direction, previous work [@strain_2023] has been unable to determine the extent
to which each is responsible for the observed reduction in correlation estimation
error.

## Point Size {#sec-point-size}

For discriminability reasons, scatterplots visualizing large datasets tend also to
have smaller points. Bubble charts are a subclass of scatterplot which uses point
size to describe a third variable, but what little experimental work there
is on the impact of point size on correlation perception is inconclusive. Some work
has found bias and variability in correlation perception to be invariant to changes in
point size [@rensink_2012; @rensink_2014], while elsewhere a strong effect of
changing point size as a function of distance from the regression line has been
reported [@strain_2023b]. Evidence points towards a salience-dominant mechanism 
in the latter case, albeit with a small effect of spatial uncertainty. There
is evidence that stimulus size is associated with lower levels of spatial certainty [@alais_2004]
but higher levels of salience [@healey_2011], and so the opposing
directionality of predicted effects of salience/perceptual weighting and spatial uncertainty
on correlation estimation has allowed researchers to provide evidence for the mechanistic
dominance of salience when point size is used.

## Hypotheses

Based on previously established effects of adjusting point size and point
contrast using identical decay functions in isolation, we presently 
make the following hypotheses about the combination of these functions. 
We hypothesize that; (H1) an increased reduction in correlation estimation error will be
observed when standard orientation decay functions are used; (H2) the use of 
congruent inverted orientation decay functions will produce 
the least accurate estimates of correlation; and (H3) that owing to the greater strength
of the size channel observed in previous work [@strain_2023b], there will be a 
significant difference in correlation estimates between the two incongruent orientation conditions.

# Methodology {#sec-methods}

## Crowdsourcing {#sec-crowdsourcing}

Much prior work into correlation perception in scatterplots has taken place
in-person, most often with graduate students with experience in statistics. While
this work is valuable, especially to perception audiences, it can struggle
to provide data that is resilient to different lay viewers and viewing contexts.
In addition, the ease and low-cost afforded to us by online, crowdsourced
experimental work is unmatched. Given our intended HCI and design audience, we therefore
choose to crowdsource all participants. We acknowledge however, that the 
technique has been affected by low quality data and skewed demographics
in the past [@chmielewski_2020; @charalambides_2021; @peer_2021]. In light of these
issues we follow published guidelines [@peer_2021] to ensure the collection of high quality 
data. Namely, we use the Prolific.co platform [@prolific] with stringent pre-screen
restrictions; participants were required to have completed at least 100 studies
using Prolific, and were required to have a prolific score of 100, representing a 99%
approval rate. This is more strict than the 95% suggested in previous work [@peer_2021],
but has served the authors well in prior studies.

## Open Research {#sec-open-research}

This study was conducted according to the principles of open and reproducible
research. All data and analysis code are available at (repository link removed for anon).
This repository contains instructions for building a docker image to fully reproduce the computational
environment used. This allows for full replications of stimuli, analysis, and the paper
itself. Ethical approval was granted by (removed for anon). Hypotheses
and analysis plans were pre-registered with the OSF (links removed for anon) and
there were no deviations from them.

## Stimuli {#sec-scatter-gen}

The data used to generate the scatterplots were identical to that used in
previous work [@strain_2023; @strain_2023b]. 45 scatterplot datasets were generated corresponding
to 45 *r* values uniformly distributed between 0.2 and 0.99, as there is evidence that
very little correlation is perceived below *r* = 0.2 [@strahan_1978; @bobko_1979; @cleveland_1982].
Using so many values for *r* allows us to paint a broader picture of people's perceptions
than work using fewer values. Scatterplot points were generated based on
bivariate normal distributions with standard deviations of 1 in each direction. Each
scatterplot had a 1:1 aspect ratio, was generated as a 1000*1000 pixel .png image,
and was scaled up or down according to a participant's monitor such that they
always occupied the same proportion of the screen. We used equation 1 to map residuals 
to size and contrast values. When adjusting point size, we further transform values
using a scaling factor of 4 and a constant of 0.2 t ensure that the minimum point
size in the present study is consistent with that of previous work [@strain_2023; @strain_2023b].

\begin{equation}
  point_{size/contrast} = 1 - b^{residual}
\end{equation}

0.25 was chosen as the value of *b*. This is both due to its previous usage in studies
that the present work builds upon, and its production of a curve approximating the
inverse around the identity line of the underestimation curve reported in previous
work [@rensink_2017; @strain_2023; @strain_2023b]. We acknowledge that there
may be other, more suitable values of *b*, however extensive testing of these is
outside the scope of the present work. We used 2x2 combinations of this equation
applied to point size and contrast in standard and inverted orientation forms. 
Examples of the stimuli used can be seen in @fig-examples.

```{r}
#| label: fig-examples
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"
#| include: true
#| fig-cap: Examples of the experimental stimuli used.

example_plots <- ggarrange(plot_example_function(slopes,
                                        (1-slopes$slope_0.25),
                                        (1-slopes$slope_0.25),
                                        "Standard Orientation Size\nStandard Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_inverted),
                                         "Inverted Orientation Size\nInverted Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_0.25),
                                         "Inverted Orientation Size\nStandard Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_0.25),
                                         (1-slopes$slope_inverted),
                                         "Standard Orientation Size\nInverted Orientation Contrast"))

example_plots
```

## Modelling {#sec-gen-modelling}

We use linear mixed effects models to model the relationships
between the combination of size and contrast decay
conditions and participants' errors in correlation estimates. Models such as these
allow us to compare differences in our IV across the full range of participant 
responses, as opposed to relying purely on aggregate data, as in ANOVA. These models also afford us 
the ability to include random effects for participants and items. As per our pre-registrations
we preferred maximal models, including random intercepts and slopes for participants
and items. The structures of these models was identified using the **buildmer** package
in R (version xx, [@voeten_buildmer]). This package takes a maximal random effects
structure and then identifies the most complex model that converges, dropping
terms that fail to explain a significant amount of variance.

## Point Visibility Testing {#sec-VT}

Discussions about the size and contrast of particular scatterplot points are inherently
difficult in the context of online, crowdsourced experiments; controlling the devices
participants use to participate in these kind of experiments, beyond insisting on laptop
or desktop computers, is impossible. While this may result in a lack of consistency
in scatterplot point sizes, luminances, or contrast ratios between participants, it also provides results
that are more resilient to different viewing contexts than traditional lab-based 
experimental work. In addition to measures
implemented to ensure high quality participant data (see @sec-crowdsourcing), it is also key that we do not
inadvertently remove data from scatterplots by including points whose size or contrast
renders them invisible. We therefore included point visibility testing to ensure this. Participants
viewed six scatterplots that were made up of a certain number of points. These points
were of the same size and contrast as the smallest and lowest contrast points used
in the experimental items. Participants were asked to enter in a textbox how many points
were present. Participants scored an average of
`r printnum(VT$mean_VT_perc_correct)`% ($SD$ = `r printnum(VT$sd_VT_perc_correct)`).
Despite our use of the contrast floor detailed in @sec-transparency-and-contrast,
it is clear that some of our small, low contrast points were not reliably visible, most likely
due to low contrast between the point and background, as previous work [@strain_2023b] 
found point visibility largely invariant to size. We suggest this is due to differences in
monitors between participants. In reality this contrast floor would need to be
calibrated on a per-monitor basis. @fig-VT-hist shows distributions of participants'
performances on the visibility tests. We also include performance on the point
visibility test as a fixed effect in @sec-add-analyses.

```{r}
#| label: fig-VT-hist
#| include: true
#| fig-asp: 0.5
#| fig-cap: Histogram of point visibility testing performance.
#| out-width: "50%"
#| fig-align: "left"

# histograms of visual threshold testing performance for

ggplot(size_and_contrast_exp_tidy, aes(x = VT_no_correct)) +
  geom_histogram(binwidth = 1,colour = "grey", fill = "darkgrey") +
  theme_ggdist() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_text(size = 14)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 9)) +
  labs(x = "Number of Visual Threshold Tests Correct (out of 6)",
       y = "Count")
```

## Dot Pitch {#sec-dot-pitch}

We employed a method for obtaining the dot pitch of participants' monitors [@screenscale].
Combining this with monitor resolution information allows us to calculate the physical on-screen
size of scatterplot points. Participants were asked to hold a standard size credit/
debit/ID card (ISO/IEC 7810 ID-1) up to their screen and resize an on-screen card
until the two matched. We assumed a widescreen 16:9 aspect ratio and calculated
dot pitch based on these measurements. Mean dot pitch was 
`r printnum(dot_pitch$mean_dp, digits = 2)` ($SD$ = `r printnum(dot_pitch$sd_dp, digits = 2)`).
We include analyses with dot pitch as a fixed effect in @sec-add-analyses.

## Procedure {#sec-gen-procedure}

Both experiments were built using PsychoPy [@pierce_2019] and hosted on Pavlovia.org.
Participants were only permitted to complete the experiment on a desktop or laptop
computer. Each participant was first shown the participant information sheet and provided
consent through key presses in response to consent statements. They were asked to
provide their age in a free text box, followed by their gender identity. Participants
completed the 5-item Subjective Graph Literacy test [@garcia_2016], followed by the
visual threshold task described in @sec-VT and the screen scale task described in 
@sec-dot-pitch. Participants were given instructions, and were then shown examples
of scatterplots with correlations of *r* = 0.2, 0.5, 0.8, and 0.95, as piloting of
a previous experiment indicated some of the lay population may be unfamiliar
with the visual character of scatterplots. @sec-results contains further
discussion of the potential training effects of this. Two practice trials were
given before the experiment began. Participants worked through a randomly
presented series of 180 experimental trials and were asked to
use a slider to estimate correlation to 2 decimal places. Visual masks preceded
each scatterplot. Interspersed were 6 attention check trials which explicitly
asked participants to ignore the scatterplot and set the slider to 0 or 1.

## Participants {#sec-participants}

150 participants were recruited using the Prolific.co platform. Normal to 
corrected-to-normal vision and English fluency were required for participation.
In addition, participants who had completed any of our previous studies
into correlation estimation in scatterplots (references removed for anon) 
were prevented from participating. Data were collected from 158 participants. 
8 failed more than 2 out of 6 attention
check questions, and, as per pre-registration stipulations, were rejected from the
study. Data from the remaining 150 participants were included in the full analysis
(`r printnum(gender$M, digits = 1)`% male, `r printnum(gender$F, digits = 1)`% female,
and `r printnum(gender$NB, digits = 1)`% non-binary).
Participants mean age was `r printnum(age$mean, digits = 1)` (*SD* = `r printnum(age$sd, digits = 1)`).
Participants' mean graph literacy score was `r printnum(literacy$mean, digits = 1)`
(*SD* = `r printnum(literacy$sd, digits = 1)`). The average
time taken to complete the experiment was 37 minutes (*SD* = 12.3).

## Design {#sec-design}

We used a fully repeated-measures 2*2 factorial design. Each participant
saw each combination of size and contrast decay condition plots for a total of 180
experimental items. Participants viewed these experimental items, along with 6
attention check items, in a fully randomized order. All experimental code, materials,
and instructions are hosted at (link removed for anon).

# Results {#sec-results}

Our first two hypotheses were fully supported in this experiment. The combination of 
standard orientation size and contrast decay functions produced the most accurate estimates of
correlation, although this also resulted in a large correlation overestimation for many
values of *r* (see @fig-diff-error-bars-plot). Our second hypothesis was also 
supported; the combination of inverted size and inverted contrast decay conditions
produced the least accurate estimates of correlation. We found no support for our third
hypothesis; there was no significant difference in correlation estimates for
standard orientation size/inverted orientation contrast decay plots and 
inverted orientation size/standard orientation contrast
decay plots (Z = -2.26, *p* = .11), however we did find a significant interaction effect
that provides evidence that the size decay function was stronger with regards
to biasing people's estimates of correlation.

```{r}
#| label: model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# r estimation error modelling

model <- buildmer(difference ~ size * contrast +
                       (1 + size * contrast | participant) +
                       (1 + size * contrast | item),
                     data = size_and_contrast_exp_tidy)
```

```{r}
#| label: assign-slot

model <- model@model
```

```{r}
#| label: model-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# buildcomparison model with both fixed effects terms removed

model_cmpr <- comparison(model)
```

```{r}
#| label: anova

# runs ANOVA between experimental and comparison models

anova_results(model, model_cmpr)
```

```{r}
#| label: fig-dot-plot
#| include: false
#| fig-cap: Mean r estimation for each combination of conditions. 95% confidence intervals are shown as error bars.
#| fig-asp: 0.6
#| out-width: 50%
#| fig-align: "left"

# dot plot of mean errors and 95% CIs for each combination of size and contrast decay conditions
# currently not used s- EMM plot used in place

dot_plot_function(size_and_contrast_exp_tidy, "Size Decay : Contrast Decay") +
    scale_x_discrete(labels = labels_size_contrast) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 9))
```

```{r}
#| label: tbl-sig
#| include: true
#| tbl-cap: Significances of fixed effects and the interaction between them.

make_sig_table(model)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`). 
Deviation coding was used for each of the experimental factors. We used the **buildmer** and
**lme4** packages to build a linear mixed effects model where the difference
between objective and rated r value was predicted by the size and contrast decay
conditions used. A likelihood ratio test revealed that the model including point size
and contrast decay conditions as fixed effects explained significantly more variance
than the null ($\chi^2$(`r in_paren(model.df)`) = 
`r printnum(model.Chisq)`, *p* `r printp(model.p, add_equals = TRUE)`). There
were significant fixed effects of size decay and contrast decay conditions, as well
as a significant interaction between the two. @tbl-sig shows a summary
of the model statistics. The experimental model has random intercepts for items
and participants, and a random slope for the size decay factor with regards to participants.

```{r}
#| label: fig-emm-plot
#| include: true
#| fig-asp: 0.4
#| out-width: "100%"
#| fig-cap: Estimated marginal means for each combination of size and contrast decay conditions, including asymptotic lower and upper confidence limits calcualted by emmeans

as_tibble(emmeans(model, pairwise ~ size * contrast)[[1]]) %>%
  mutate("size" = recode(size,
                         "S" = "Standard Orientation Size",
                         "I" = "Inverted Orientation Size"),
          "contrast" = recode(contrast,
                         "S" = "Standard Orientation Contrast",
                         "I" = "Inverted Orientation Contrast")) %>%
  ggplot(aes(x = size:contrast, y = emmean)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL, ymax = asymp.UCL)) +
  scale_x_discrete(labels = ~ gsub(":", "\n", .x)) +
  theme_ggdist() + 
  labs(y = "Estimated Marginal Mean of Error",
       x = " Size : Contrast") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(length = unit(0.1, "cm"))) +
  annotate(geom = "text", x = 4.15, y = -.05, label = "Overestimation", size = 3) +
  annotate(geom = "text", x = 4.15, y = .05, label = "Underestimation", size = 3)
  
```

```{r}
#| label: fig-interaction-plot
#| include: false
#| fig-cap: Interaction plot showing the moderating effect of encoding channel on decay orientation.
#| fig-asp: 0.7
#| fig-align: "left"
#| #| out-width: 50%

# not used for manuscript

emm <- emmeans(model, ~ size * contrast)

emm %>%
  as_tibble() %>%
  mutate("contrast" = recode(contrast,
                             "I" = "Inverted Decay",
                             "N" = "Non-linear Decay")) %>%
  ggplot() +
    aes(x = size, y = emmean, colour = contrast) +
    geom_line(aes(group = contrast), size = 1) +
    geom_point(size = 3) +
    theme_ggdist() +
    ylim(-0.1,0.175) +
    scale_x_discrete(labels = c("Non-Linear Decay", "Inverted Decay")) +
    labs(x = "Size Decay Condition",
         y = "Estimated Marginal Mean",
         colour = "Contrast\nDecay\nCondition") +
  theme(legend.position = c(0.75,0.3),
        axis.text = element_text(size = 14),
        axis.title = element_text(size = 16))
```

```{r}
#| label: tbl-contrasts
#| include: true
#| tbl-cap: Pairwise comparisons for experiment 1. Our interaction is driven by the greater strength of the size channel, whether non-linear or inverted decay functions were used. Note the lack of significance for NL size/Inv contrast against Inv size/NL contrast comparison.

table_df <- contrasts_extract(model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  mutate('Contrast' = recode(Contrast,
                             "S I - I I" = "Non-linear Size x Inverted Contrast <-> Inverted Size x Inverted Contrast",
                             "S I - S S" = "Non-linear Size x Inverted Contrast <-> Non-Linear Size x Non-linear Contrast",
                             "S I - I S" = "Non-linear Size x Inverted Contrast <-> Inverted Size x Non-linear Contrast",
                             "I I - S S" = "Inverted Size x Inverted Contrast <-> Non-linear Size x Non-linear Contrast",
                             "I I - I S" = "Inverted Size x Inverted Contrast <-> Inverted Size x Non-linear Contrast",
                             "S S - I S" = "Non-linear Size x Non-linear Contrast <-> Inverted Size x Non-linear Contrast"))

kable(table_df, booktabs = TRUE, digits = c(0,2,3))
```

```{r}
#| label: fig-diff-error-bars-plot
#| include: true
#| out-width: 100%
#| fig-cap: Plots showing how participants' correlation estimation errors change as a function of the *r* value for each combination of size and contrast decay factors.
#| fig-asp: 1

plot_error_bars_function(size_and_contrast_exp_tidy %>%
                        mutate(condition_abs = fct_relevel(condition_abs,
                                                          c("A", "B", "X", "Y"))),
                        "difference",
                        labels_size_contrast) +
  geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: additional-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

lit_model <- lmer(add.terms(formula(model), "literacy"),
                  data = size_and_contrast_exp_tidy)

anova_results(lit_model, model)

VT_model <- lmer(add.terms(formula(model), "VT_no_correct"),
                  data = size_and_contrast_exp_tidy)

anova_results(VT_model, model)

dot_pitch_model <- lmer(add.terms(formula(model), "dot_pitch"),
                        data = size_and_contrast_exp_tidy)

anova_results(dot_pitch_model, model)
```

## Additional Analyses {#sec-add-analyses}

We find no effects of graph literacy ($\chi^2$(`r in_paren(lit_model.df)`) = 
`r printnum(lit_model.Chisq)`, *p* `r printp(lit_model.p, add_equals = TRUE)`) or
performance on the visual threshold task ($\chi^2$(`r in_paren(VT_model.df)`) = 
`r printnum(VT_model.Chisq)`, *p* `r printp(VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(dot_pitch_model.df)`) = `r printnum(dot_pitch_model.Chisq)`,
*p* `r printp(dot_pitch_model.p, add_equals = TRUE)`) on participants' errors in correlation estimation.

# Discussion {#sec-discussion}

Our findings here provide further confirmatory evidence of what has been found
previously with regards to the effects of point size and contrast manipulations
on correlation estimation in scatterplots. Namely, that while both manipulations
have a significant effect, the effect of changing point sizes is stronger, and that
while we can influence correlation estimates in either direction, standard
orientation manipulations are more powerful than inverted ones. The experiment
described above additionally contributes evidence of a congruency effect; effects
are most strong when there is orientation congruency between size and contrast
manipulations. 

The lack of support for our third hypothesis, that there would be a
difference in correlation estimates between incongruent conditions, was surprising
given the greater strength of the size channel relative to contrast. We take
this as evidence that the combination of size and contrast manipulations is not
additive, as we had initially suspected, which explains the interaction we see here.
Integrating the data collected in the present study with previously collected
data investigating the effects of the size and contrast manipulations in isolation
allows us demonstrate this and to partial out the effects each manipulation.

```{r}
#| label: fig-error-bars-all-exp
#| include: true
#| out-width: "100%"
#| fig-cap: From left to right, plotting *r* estimation error against the objective *r* value for the standard orientation condition in the present study, for standard orientation size and contrast manipulations in previous work, and for normal scatterplots averaged over identical conditions in previous work. Error bars have been left off this plot to make interpretation more simple.

# dataframe containing values from previous experiments and the current is included
# in the data folder 

all_exp_df <- read_csv("data/all_exp.csv")
  
  all_exp_df %>% 
    drop_na() %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    #geom_point(size = 0.2) + 
    #geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 4, labeller = labeller(factor = labels_all_exp)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: fig-power-plot
#| include: true
#| out-width: "100%"
#| fig-cap: additive_raw_pl = observed values for present study. standard_curve = no manipulation averaged across all experiments

# dataframe containing values from previous experiments and the current is included
# in the data folder 
curves <- read_csv("data/curves_df.csv") 

  curves %>% 
    drop_na() %>%
    select(c("contrast_power", "size_power", "additive_power", "my_rs")) %>%
    pivot_longer(cols = c("contrast_power",
                          "size_power",
                          "additive_power"),
                 names_to = "factor", values_to = "power") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(power), mean = mean(power)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 5, labeller = labeller(factor = labels_power)) +
    labs(x = "Objective r",
         y = "Power") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    ylim(-0.1,0.3)


```

As can be seen in @fig-error-bars-all-exp, combining the size and contrast manipulations
results in an error curve of a mostly similar shape to that of the size manipulation.
The difference that comes from adding the contrast manipulation lies in the severity
of the effect and the orientation of the curve itself. Transforming the size underestimation
curve from @strain_2023 into the size and contrast underestimation curve we have
found in the present study gives us some insight into the effect of adding 
the two manipulations together.


# References {-}


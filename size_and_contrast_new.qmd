---
format: docx

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
keep-tex: true

bibliography: size-contrast-new.bib

params:
  eval_models: false
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

title: Novel Effects of Size and Contrast Adjustments in Scatterplots

# if short-title is defined, then it's used
short-title: Size, Contrast, and Scatterplots

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: manuscript, anonymous, screen  

  # acm preamble information
  copyright-year: 2018
  acm-year: 2018
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation emai
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
     <concept>
      <concept_id>10010520.10010553.10010562</concept_id>
      <concept_desc>Computer systems organization~Embedded systems</concept_desc>
      <concept_significance>500</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010575.10010755</concept_id>
      <concept_desc>Computer systems organization~Redundancy</concept_desc>
      <concept_significance>300</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010553.10010554</concept_id>
      <concept_desc>Computer systems organization~Robotics</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
     <concept>
      <concept_id>10003033.10003083.10003095</concept_id>
      <concept_desc>Networks~Network reliability</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
    </ccs2012>
    \end{CCSXML}
    
    \ccsdesc[500]{Computer systems organization~Embedded systems}
    \ccsdesc[300]{Computer systems organization~Redundancy}
    \ccsdesc{Computer systems organization~Robotics}
    \ccsdesc[100]{Networks~Network reliability}

  keywords:
    - correlation
    - scatterplot
    - perception
    - crowdsourced

abstract: |
  Changing the size and contrast of points on scatterplots can be used to systematically improve viewers' perceptions of correlation. Evidence points to these effects being similar with regards to the mechanisms behind them, so one would expect that their combination would produce a simple additive effect on correlation estimation. We present a fully  reproducible study in which we combine techniques for influencing correlation perception to show that in reality, effects of changing point size and contrast interact in a non-additive fashion. We show that there are few limits to the extent to which we can use visual features to change viewers' perceptions of data visualizations, and use our results to further explain the perceptual mechanisms at play when changing point size and contrast in scatterplots.
  
---
```{r}
#| label: setup

set.seed(1234) # seed for all random number generation

# Loading packages

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(EMAtools)
library(ggtext)
library(r2glmm)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("size_and_contrast_new_cache/pdf") }
```

```{r}
#| label: load-data

# load in data file
# Need higher guess_max so that read_csv() guesses column types correctly 

additive_anon <- read_csv("data/additive_data.csv", guess_max = 18001)
# tuning_anon <- read_csv("data/tuning_data.csv")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling function *must* be run first to make
## the data set usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels",
             "participant",
             "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels,
                                   pattern = "vis_threshold_plots/",
                                   replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer,
                                   pattern = "_VT.png",
                                   replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    mutate(VT_perc_correct = (VT_no_correct/6)*100) %>%
    select("VT_perc_correct",
           "VT_no_correct",
           "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
monitor_information <- anon_file %>%
  mutate(height = dplyr::lead(height)) %>%
  mutate(res_height = res_width*0.5625,
         width = height*0.5625,
         dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
         select(c("dot_pitch",
                  "participant",
                  "res_width")) %>%
  na.omit()
  
# extract demographic information
# link slider response numbers to gender categories
  
demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                    "age_textbox.text",
                    "gender_slider.response")))

# split images column into item and condition columns
# additionally create "condition_abs" column
# this will be simpler to plot with later
# and allows us to use wrangle function for both experiments

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-S-S")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-I-I")) %>%
  mutate(images = str_replace(images, pattern = "X", replacement = "-S-I")) %>%
  mutate(images = str_replace(images, pattern = "Y", replacement = "-I-S")) %>%
  separate(images, c("item",
                     "contrast",
                     "size"),
           sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(condition_abs = case_when(
    contrast == "S" & size == "S" ~ "A",
    contrast == "I" & size == "I" ~ "B",
    contrast == "S" & size == "I" ~ "X",
    contrast == "I" & size == "S" ~ "Y",
    TRUE ~ "placeholder"
  ))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
            "item",
            "size",
            "contrast",
            "slider.response",
            "my_rs",
            "total_residuals",
            "unique_item_no",
            "session",
            "trials.thisN",
            "condition_abs")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>% # used for training testing later on
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "contrast", "size", "condition_abs")), as_factor)) %>%
  mutate(difference = my_rs - slider.response) %>%
  select(-c("__participant")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(additive_anon)

# set deviation coding for experimental model

contrasts(size_and_contrast_exp_tidy$size) <- matrix(c(.5, -.5))
contrasts(size_and_contrast_exp_tidy$contrast) <- matrix(c(.5, -.5))

# remove anon df from environment

rm(additive_anon)

# extract age data

age <- distinct(size_and_contrast_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

  sum(is.na(size_and_contrast_exp_tidy$age_textbox.text))
  
# extract gender data

gender <- distinct(size_and_contrast_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(size_and_contrast_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract visual threshold data

VT <- size_and_contrast_exp_tidy %>%
  summarise(mean_VT_no_correct = mean(VT_no_correct),
            sd_VT_no_correct = sd(VT_no_correct),
            mean_VT_perc_correct = mean(VT_perc_correct),
            sd_VT_perc_correct = sd(VT_perc_correct))

# extract dot pitch data

dot_pitch <- size_and_contrast_exp_tidy %>%
  summarise(mean_dp = mean(dot_pitch),
            sd_dp = sd(dot_pitch))
```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# termS removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size * contrast)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
  
}
```

```{r}
#| label: dot-plot-function

# function to create dot plots showing average correlation estimation 
# errors and 95% CIs

dot_plot_function <- function(df, xlabel) {

data <- df %>%
  group_by(condition_abs) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(condition_abs)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1], # lower confidence interval
    hci = t.test(difference, conf.level = 0.95)$conf.int[2], # upper confidence interval
  )

  data %>%
    mutate(condition_abs = fct_reorder(condition_abs, mean)) %>%
    ggplot(aes(x = condition_abs, y = mean)) +
    geom_point(stat = "identity", size = 1) +
    geom_errorbar(aes(ymin=lci, ymax=hci), colour="black", width=0.01, linewidth=0.5) +
    theme_ggdist() +
    labs(x = xlabel,
         y = "Mean Error") +
    theme(axis.text = element_text(size = 14),
          axis.title = element_text(size = 16))
}
```

```{r}
#| label: error-bar-plot

# plot the error bars plots by condition
# takes dataframe, measure (i.e difference or raw r score), and label vector

plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(condition_abs, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),
                width = 0.01,
                size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
  theme(strip.text = element_text(size = 6,
                                  margin = margin(1,0,1,0, "mm")),
        aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  facet_wrap(condition_abs ~., ncol = 4, labeller = labeller(condition_abs = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}
```

```{r}
#| label: labelling-function

# creates labels vector for use with plotting functions

labels_size_contrast <- c(A = "Standard Orientation Size\nStandard Orientation Contrast",
                          B = "Inverted Orientation Size\nInverted Orientation Contrast",
                          X = "Standard Orientation Size\nInverted Orientation Contrast",
                          Y = "Inverted Orientation Size\nStandard Orientation Contrast")

labels_all_exp <- c(additive_manipulation = "Size and Contrast Manipulated",
                    contrast_manipulated = "Contrast Manipulated",
                    size_manipulated = "Size Manipulated",
                    standard_plot = "No Manipulation Present")

labels_power <- c(size_power = "Size\nPower",
                  contrast_power = "Contrast\nPower",
                  additive_power = "Size and Contrast\nCombined Power")
```

```{r}
#| label: make-sig-table-lm

# function to create table with fixed effects and interactions for LMM

make_sig_table <- function (model) {
  
  # buildmer class models need reassigned to Lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  table_df <- as.data.frame(summary(model)[10]) %>%
    rename("Estimate" = "coefficients.Estimate",
           "Standard Error" = "coefficients.Std..Error",
           "df" = "coefficients.df",
           "t-value" = "coefficients.t.value",
           "p" = "coefficients.Pr...t..") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p")
  
  # get semi-partial r^2, rename rows so they match with sig table
  # take the last 3 rows corresponding to fixed effects and interaction term
  # remove first cell of r^2
  # reformat text for latex output
  
  r_squared <- r2beta(model, method = "nsj") %>%
    mutate("Effect" = recode(Effect,
                             "size" = "Size Decay",
                             "contrast" = "Contrast Decay",
                             "size:contrast" = "Size Decay x Contrast Decay")) %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
  # tidy up row names
  
  rownames(table_df) <- c("(Intercept)",
                          "Size Decay",
                          "Contrast Decay",
                          "Size Decay x Contrast Decay")
  
  sig_and_squared <- cbind(table_df, r_squared) %>% select(-c("Effect")) 
  
  table <- kable(sig_and_squared, booktabs = TRUE, digits = c(2,3,2,2,2,2,3), escape = FALSE)
  
  return(table)
}
```

```{r}
#| label: plot-examples-function
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
  mutate(slope_inverted_floored = pmax(0.1,(1+(0.25)^my_residuals)-1)) 
  
plot_example_function <- function (data, size, contrast, title) {
  
set.seed(1234)
  
  ggplot(data, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(size + 0.2),
                 alpha = contrast),
             shape = 16)  +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 13),
        axis.title = element_blank()) +
  labs(title = title)

} 
```

# Introduction

Scatterplots are common bivariate representations of data. They have been extensively
studied and are used for a variety of communicative tasks. While most commonly
used to represent linear correlation, or the degree of linear relatedness between two variables,
they can also be used to represent different groups (clustering), to aid in the detection of outliers,
to characterize distributions, and to visualize non-linear correlations.
@fig-tasks contains examples of scatterplots optimized for different tasks. There
is evidence that people generally interpret them in similar ways [@kay_2015], and
that they support the interpretation of correlation significantly better than other
data visualizations [@li_2010]. Rapid interpretation by viewers [@rensink_2014] along
with low levels of interindividual variance render scatterplots particularly suited
for experimental work; they provide important insights into perception and visualization design while
being simple to study [@rensink_2014]. 

While our interpretations of scatterplots are generally similar, the accuracy of those
interpretations is generally low. Viewers systematically underestimate the correlation
displayed in positively correlated scatterplots. This holds true for direct 
estimation tasks [@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989;
@collyer_1990; @meyer_1992], and estimation via bisection tasks [@rensink_2017], and
is particularly pronounced between 0.2 < *r* < 0.6. The COVID-19 pandemic demonstrated
that even outside of professional/office environments, lay 
populations are now expected to be able to use and accurately interpret data
visualizations on a daily basis [@bbc_2022]. This expectation confers a responsibility on
the part of data visualization designers to design in such a way that people with
little statistical or graph training can expect to be able to correctly interpret
data visualizations. Doing this requires us to understand human perception and
perceptual phenomena, apply this understanding to data visualization design, and
test those designs in rigorous empirical work. We therefore present a fully-reproducible,
crowdsourced online experiment in which we systematically alter visual features
in scatterplots to correct for a historic bias. We combine two techniques for
correcting for correlation underestimation in scatterplots and show that the combined
effect is stronger than one would expect were they linearly additive. Through
this work we also present a framework for visualization design 
informed from the ground up by human perception.

```{r}
#| label: fig-tasks
#| include: true
#| out-width: "100%"
#| fig-asp: 0.25
#| fig-cap: Examples of scatterplots designed for different scatterplot-associated tasks. Both colour and point shape have been used to delineate different clusters in the cluster separation plot.

# set random seed for reproducibility

set.seed(123)

# create theme so that newly generated plots match the plot example function

tasks_theme <- function() {
               theme_classic() %+replace%
               theme(axis.text = element_blank(),
               plot.margin = unit(c(0,0,0,0), "cm"),
               legend.position = "none",
               plot.title = element_text(size = 7.5, hjust = 0, vjust = 1),
               axis.title = element_blank(),
               axis.line = element_line(linewidth = 0.25),
               axis.ticks = element_line(linewidth = 0.25),
               axis.ticks.length = unit(1.375, "pt"))
}

## Create correlation plot

corr_plot <- plot_example_function(slopes,
                      0.05,
                      0,
                      "Correlation\nPerception") +
  tasks_theme() +
  geom_point(aes(alpha = 1), size = 0.5) +
  annotate(geom = "text", x = -2, y = 2, label = "r = 0.6", size = 3.5)

## Create cluster separation plot

# cluster sep data

n <- 128

cluster_sep_data <- data.frame(
  x = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 8, sd = 1)),
  y = c(rnorm(n, mean = 0, sd = 1), rnorm(n, mean = 4, sd = 1), rnorm(n, mean = 0, sd = 1)),
  cluster = factor(rep(1:3, each = n)))

# create plot

cluster_sep_plot <- ggplot(cluster_sep_data, aes(x = x, y = y, colour = cluster)) +
  geom_point(size = 0.6, aes(shape = cluster)) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Cluster\nSeparation")

## Outlier detection plot

# make data

n <- 127
x <- runif(n, 0, 100)
y <- 2 * x + 5 + rnorm(n, 0, 10) 
x <- c(x, 35) 
y <- c(y, 200) 

data <- data.frame(x, y)

# create plot

outlier_plot <- ggplot(data, aes(x, y)) +
  geom_point(size = 0.2) +
  scale_size_identity() +
  tasks_theme() +
  labs(title = "Scatterplot with\nObvious Outlier") +
  ylim(0,300)

## Non-linear correlation plot

# make data
  
V1 <- seq(-10, 10, length.out = 128)

V2 <- 2 * V1^2 + rnorm(length(V1), mean = 0, sd = 10)

NL_data <- data.frame(V1, V2)

# make plot

NL_corr_plot <- plot_example_function(NL_data,
                                      0.05,
                                      0,
                                      "Non-linear\ncorrelation") +
  tasks_theme() +
  geom_point(aes(alpha = 1),
             size = 0.2)

# arrange the four task example plots

ggarrange(corr_plot,
          cluster_sep_plot,
          outlier_plot,
          NL_corr_plot,
          nrow = 1)


```

# Related Work {#sec-related-work}

## Testing Correlation Perception {#sec-testing-corr-percept}

The testing of linear correlation perception in scatterplots has a long and rich history, and has
explored a wide variety of plot types, tasks, and modeling methods. Work 
has had participants make discriminative judgements between scatterplots with different
correlations [@pollack_1960; @doherty_2007], finding that performance on such a task became better
as the objective *r* value increased. This performance on a discriminative judgement
task can also be modeled by deep neural networks [@yang_2023]. Extensive work throughout the 1970's to 1990's 
focused primarily on having participants produce a numerical estimate of correlation,
and found evidence for a systematic underestimation for positive *r* values besides
0 and 1. This underestimation was especially pronounced for 0.2 < *r* < 0.6
[@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989; @collyer_1990; @meyer_1992]
and is demonstrated using an equation relating objective to subjective correlation [@rensink_2017] in
@fig-underestimation-curve. More recent work has attempted to model participants' correlation estimation performance
by using a combination of a bisection task, in which participants are asked to adjust a
plot until its correlation is halfway between that of two reference plots, and a staircase
task designed to produce Just Noticeable Differences between scatterplots such
that they are discriminable 75% of the time [@rensink_2010]. This work has
also been extended to incorporate Bayesian data analysis [@kay_2015]. The current experiment
takes similar techniques from previous work [@strain_2023; @strain_2023b] and 
combines them to further push the envelope of how systematically adjusting visual
features in scatterplots can radically alter people's perceptions of correlation.
For this reason we use the same direct estimation paradigm  to collect responses.
This paradigm allows for a large number of judgements to be collected,
and is simple enough that participants need little to no training.

## Drivers of Correlation Perception {#sec-drivers}

Evidence points towards correlation perception being driven by the shape of the
underlying probability distribution represented by scatterplot points, however it should
be noted that this is very much still an open question, especially with regards to
the low level perceptual mechanisms at play. It may be the case that
there are different contributory perceptual mechanisms operating at different levels
based on task-specific differences such as viewing time or levels of graph-training.
Increasing the *x* and *y* scales on a scatterplot such that the size of the point
cloud decreases [@cleveland_1982] is associated with an increase in a viewer's
judgements of bivariate association, despite the objective *r* value remaining the same. It
was suggested in this case that viewers may have been using the area of the point
cloud to judge association. Later work found that the relationship between objective
and perceived *r* values could be described by a function that included the mean of the
geometric distances between the points and the regression line [@meyer_1997]. Investigation
of the idea that people use visual features to judge correlation provides
evidence that, among others features, the standard deviation of all perpendicular
distances from scatterplot points to the regression line is predictive of performance
on a correlation estimation task [@yang_2019]. Equations for both discrimination
and magnitude estimation of *r* in scatterplots include a quantity that is small
when *r* = 1 and increases as *r* approaches 0 [@rensink_2017]. This quantity is indifferent
to the type of visualization used, and is functionally similar to that found
in work mentioned above [@cleveland_1982; @meyer_1997; @yang_2019]. Regarding
scatterplots, this quantity represents the average distance between data points
and the regression line, and can be thought of as representing the width
of the underlying probability distribution. Findings from a convolutional neural network that
learnt visual features related to correlation perception also support 
the idea that viewers are using an aspect of scatterplot shape to judge correlation,
or some measure of what has been termed *dot entropy* [@yang_2023], again considered a candidate
visual proxy for correlation judgements [@rensink_2017; @rensink_2022].

Recent work investigating the use of decay functions that change point size or contrast
in scatterplots as a function of residual distance provide further evidence for 
both point density and salience/perceptual weighting being drivers of correlation perception.
The use of an inverted contrast decay function [@strain_2023], such that the contrast of a point
decreased the closer it was to the regression line, resulted in significantly
lower and less accurate correlation estimates compared to data-identical plots
with the same overall shape. This findings implies that the center of the scatterplot not
being **filled**  biased viewers' perceptions down. When point contrast
or size are reduced as a function of distance from the regression line 
[@strain_2023; @strain_2023b], viewers rate correlation as significantly higher
and are significantly more accurate, which supports a low-level data salience
account. It is more difficult to comment on higher level perceptual mechanisms,
and in any case this is not the intended contribution of the present work; we aim
to test the impact of systematically altering visual features on correlation perception, 
and to provide empirically-derived tools for visualization designers to design better
visualizations through the use of a simple, reproducible framework.

```{r}
#| label: fig-underestimation-curve
#| include: true
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"
#| fig-cap: Using a function relating objective to perceived *r* value [@rensink_2017] provides a visualization of the nature of correlation underestimation reported in previous work. An identity line has been included to illustrate where viewers are most and least accurate.

my_rs <- seq(from = 0, to = 1, length.out = 100)

as_tibble(my_rs) %>%
  mutate(under_est = (log(1-0.88*my_rs)/log(1-0.88))) %>%
  ggplot() +
  geom_smooth(aes(x = my_rs, y = under_est), colour = "black") +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  ylim(0,1) +
  xlim(0,1) +
  theme_ggdist() +
  theme(axis.title.x = ggtext::element_markdown(size = 16),
        axis.title.y = ggtext::element_markdown(size = 16),
        axis.text = element_text(size = 14)) +
  labs(x = "Objective *r* value",
       y = "Subjective *r* value")

```

## Transparency and Contrast {#sec-transparency-and-contrast}

```{r}
#| label: fig-overplotting-examples
#| include: true
#| fig-asp: 0.75
#| out-width: "50%"
#| fig-align: "left"
#| fig-cap: Adjusting point contrast to address overplotting. Contrast between the points and the background is full (alpha = 1, L) or low (alpha = .1, R). The dataset used has 40,000 points.
 
set.seed(123)

data <- data.frame(x = c(rnorm(20000, mean = -1),
                         rnorm(20000, mean = 1)),
                   y = rnorm(40000))

ggplot(data, aes(x = x, y = y)) +
  scale_alpha_identity() +
  geom_point(aes(alpha = ifelse(x > -0.1 & x < 0.1, 0,
                         ifelse(x > 0, 0.1, 1))),
             shape = 16) +
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  theme_ggdist() +
  labs(x = "",
       y = "") +
  theme(axis.text = element_blank())
```

```{r}
#| label: fig-alpha-examples
#| include: true
#| out-width: "50%"
#| fig-cap: Demonstrating the effects of different alpha values on point contrast.
#| fig-height: 2
#| fig-align: "left"

x1 <- seq(0,1, length.out = 11)
y1 <- rep(1, times = 11)
df <- data.frame(x1, y1)
ggplot(aes(x1, y1), data = df) +
  geom_point(alpha = x1, size = 18, shape = 16) +
  theme(panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(size = 16),
        axis.text.x = element_text(size = 14)) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 11)) +
rm(x1,y1,df)
```

Changing the contrast of scatterplot points is standard practice to deal with
issues of overplotting or clutter [@matejka_2015; @bertini_2004]; scatterplots
with very large numbers of points, especially with high degrees of overlap, suffer
from low individual-point visibility caused by high point density. Lowering
the contrast of all points addresses this, and makes data trends and distributions
easier to see and interpret (see @fig-overplotting-examples).
In the present study we use the **ggplot2** package [@hadley_gg2016] to create
our stimuli. This package uses an alpha parameter, or the level of linear
interpolation [@stone_2008] between foreground and background pixel values, to
set the contrast of points. As demonstrated in @fig-alpha-examples, an alpha value
of 0 or 1 results in no interpolation and rendering of either the background or foreground
pixel values respectively. Psychophysical definitions of perceived contrast are
often based on what is being presented (i.e gratings) or are modeled to take
into account phenomena of human vision (i.e visibility limits) [@zuffi_2007].
Our crowdsourced methodology gives us no control over the exact luminances
of our stimuli, only over the *relative* differences in luminance between scatterplot
points and backgrounds. For this reason we do not report absolute luminance nor
make any attempt to adopt a formal definition of contrast; instead we simply
report the alpha value used. Given that our work aims to improve correlation perception
without removing data from the scatterplot, we also incorporate point visibility testing 
(see @sec-VT). Informal point visibility piloting suggested that our smallest,
lowest contrast points had very low visibility. We therefore implemented an alpha = 0.2 floor
for these points, which we felt conferred a sufficient level of point visibility.

Previous work has found that lowering the contrast of *all* 
scatterplot points relative to the background can increase the level of
underestimation error relative to full contrast, and that lowering point
contrast *as a function of distance from the regression line* is able to bias
correlation estimates upwards to partially correct for the underestimation 
observed [@strain_2023]. Evidence suggests point salience/perceptual weighting or
spatial uncertainty as drivers for this effect. Lower stimulus contrast is
associated with lower salience [@healey_2011], can bias judgements of mean point
position [@hong_2021] and increase error in positional judgements [@wehrhahn_1990],
and can result in greater uncertainty in speed perception [@champion_2017]. Due 
to effects of both salience/perceptual weighting and spatial uncertainty operating
in the same direction, previous work [@strain_2023] has been unable to determine the extent
to which each is responsible for the observed reduction in correlation estimation
error.

## Point Size {#sec-point-size}

For discriminability reasons, scatterplots visualizing large datasets tend also to
have smaller points. Bubble charts are a subclass of scatterplot which uses point
size to describe a third variable, but what little experimental work there
is on the impact of point size on correlation perception is inconclusive. Some work
has found bias and variability in correlation perception to be invariant to changes in
point size [@rensink_2012; @rensink_2014], while elsewhere a strong effect of
changing point size as a function of distance from the regression line has been
reported [@strain_2023b]. Evidence points towards a salience-dominant mechanism 
in the latter case, albeit with a small effect of spatial uncertainty. There
is evidence that larger stimulus size is associated with lower levels of spatial certainty [@alais_2004]
but higher levels of salience [@healey_2011]. This opposing
directionality of predicted effects of salience/perceptual weighting and spatial uncertainty
on correlation estimation has allowed researchers to provide evidence for the mechanistic
dominance of salience when point size is used. When an inverted 
size decay function is used such that smaller points are located nearer the regression
line, correlation estimation is significantly more accurate than when all points
are the same size [@strain_2023b]. In this case it was suggested that the higher
spatial uncertainty brought on by larger points caused a perceptual downweighting
during correlation estimation, which is in line with work suggesting our perceptual
systems make robust use of visuo-spatial information [@strain_2023b; @warren_2002; @warren_2004].
This effect was small, meaning we do not take it into account when making
our hypotheses.

## Hypotheses

We present a single experiment based on previously established effects of 
adjusting point size and point contrast. In it we combine point size and point
contrast decay functions in both standard orientation (contrast/size is reduced
with residual magnitude) and inverted orientation (contrast/size is increased with
residual magnitude). Throughout we refer to *congruent* and *incongruent* conditions
with respect to the orientations of size and contrast decay functions. 
We hypothesize that; (H1) an increased reduction in correlation estimation error will be
observed when standard orientation decay functions are used; (H2) the use of 
congruent inverted orientation decay functions will produce 
the least accurate estimates of correlation; and (H3) that owing to the greater strength
of the size channel observed in previous work [@strain_2023b], there will be a 
significant difference in correlation estimates between the two incongruent orientation conditions.

# Methodology {#sec-methods}

## Crowdsourcing {#sec-crowdsourcing}

Much prior work into correlation perception in scatterplots has taken place
in-person, most often with graduate students with experience in statistics. While
this work is valuable, especially to perception audiences, it can struggle
to provide data that is resilient to different viewing contexts and the wide
range of levels of statistical and graph experience present in lay populations.
In addition, the ease and low-cost afforded to us by online, crowdsourced
experimental work is unmatched. Given our intended HCI and design audience, we therefore
choose to crowdsource all participants. We acknowledge however, that the 
technique has been affected by low quality data and skewed demographics
in the past [@chmielewski_2020; @charalambides_2021; @peer_2021]. In light of these
issues we follow published guidelines [@peer_2021] to ensure the collection of high quality 
data. Namely, we use the Prolific.co platform [@prolific] with stringent pre-screen
restrictions; participants were required to have completed at least 100 studies
using Prolific, and were required to have a prolific score of 100, representing a 99%
approval rate. This is more strict than the 95% suggested in previous work [@peer_2021],
but has served the authors well in prior work.

## Open Research {#sec-open-research}

This study was conducted according to the principles of open and reproducible
research. All data and analysis code are available at (repository link removed for anon).
This repository contains instructions for building a docker image to fully reproduce the computational
environment used. This allows for full replications of stimuli, analysis, and the paper
itself. Ethical approval was granted by (removed for anon). Hypotheses
and analysis plans were pre-registered with the OSF (links removed for anon) and
there were no deviations from them.

## Stimuli {#sec-scatter-gen}

45 scatterplot datasets were generated corresponding
to 45 *r* values uniformly distributed between 0.2 and 0.99, as there is evidence that
very little correlation is perceived below *r* = 0.2 [@strahan_1978; @bobko_1979; @cleveland_1982].
Using so many values for *r* allows us to paint a broader picture of people's perception
than work using fewer values. Scatterplot points were generated based on
bivariate normal distributions with standard deviations of 1 in each direction. Each
scatterplot had a 1:1 aspect ratio, was generated as a 1000 x 1000 pixel .png image,
and was scaled up or down according to a participant's monitor such that they
always occupied the same proportion of the screen. We used equation 1 to map residuals 
to size and contrast values. When adjusting point size, we further transform values
using a scaling factor of 4 and a constant of 0.2 to ensure that the minimum point
size in the present study is both visible and consistent with that of previous work [@strain_2023; @strain_2023b].

\begin{equation}
  point_{size/contrast} = 1 - b^{residual}
\end{equation}

0.25 was chosen as the value of *b*. This is both due to its previous usage in studies
that the present work builds upon, and its production of a curve approximating the
inverse around the identity line of the underestimation curve reported in previous
work [@rensink_2017; @strain_2023; @strain_2023b]. We acknowledge that there
may be other, more suitable values of *b*, however extensive testing of these is
outside the scope of the present work. We used 2x2 combinations of this equation
applied to point size and contrast in standard and inverted orientation forms. 
Examples of the stimuli used can be seen in @fig-examples.

```{r}
#| label: fig-examples
#| out-width: "50%"
#| fig-asp: 1
#| fig-align: "left"
#| include: true
#| fig-cap: Examples of the experimental stimuli used. Congruent conditions are at the top, while incongruent conditions are below.

example_plots <- ggarrange(plot_example_function(slopes,
                                        (1-slopes$slope_0.25),
                                        (1-slopes$slope_0.25),
                                        "Standard Orientation Size\nStandard Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_inverted),
                                         "Inverted Orientation Size\nInverted Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_inverted),
                                         (1-slopes$slope_0.25),
                                         "Inverted Orientation Size\nStandard Orientation Contrast"),
                   plot_example_function(slopes,
                                         (1-slopes$slope_0.25),
                                         (1-slopes$slope_inverted),
                                         "Standard Orientation Size\nInverted Orientation Contrast"))

example_plots
```

## Modeling {#sec-gen-modelling}

We use linear mixed effects models to model the relationships
between the combination of size and contrast decay
conditions and participants' errors in correlation estimates. Models such as these
allow us to compare differences in our IV across the full range of participant 
responses, as opposed to relying purely on aggregate data, as in ANOVA. These models also afford us 
the ability to include random effects for participants and items. As per our pre-registrations
we preferred maximal models, including random intercepts and slopes for participants
and items. The structures of these models were identified using the **buildmer** package
in R (version 2.10.1, [@voeten_buildmer]). This package takes a maximal random effects
structure and then identifies the most complex model that converges by dropping
terms that fail to explain a significant amount of variance.

## Point Visibility Testing {#sec-VT}

Discussions about the size and contrast of particular scatterplot points are inherently
difficult in the context of online, crowdsourced experiments; controlling the devices
participants use to participate in these kinds of experiments, beyond insisting on laptop
or desktop computers, is impossible. While this may result in a lack of consistency
in scatterplot point sizes, luminances, or contrast ratios between participants, it also provides results
that are more resilient to different viewing contexts than traditional lab-based 
experimental work. In addition to measures
implemented to ensure high quality participant data (see @sec-crowdsourcing), it is also key that we do not
inadvertently remove data from scatterplots by including points whose size or contrast
renders them invisible. We therefore included point visibility testing to check this. Participants
viewed six scatterplots that were made up of a certain number of points. These points
were of the same size and contrast as the smallest and lowest contrast points used
in the experimental items. Participants were asked to enter in a textbox how many points
were present. Participants scored an average of
`r printnum(VT$mean_VT_perc_correct)`% ($SD$ = `r printnum(VT$sd_VT_perc_correct)`).
Despite our use of the contrast floor detailed in @sec-transparency-and-contrast,
it is clear that some of our small, low contrast points were not reliably visible, most likely
due to low contrast between the point and background, as previous work [@strain_2023b] 
found point visibility largely invariant to size. We suggest this is due to differences in
monitor specifications between participants. In reality minimum visible size and contrast
would need to be calibrated on a per-monitor basis. We also include performance on the point
visibility test as a fixed effect in @sec-add-analyses.

```{r}
#| label: fig-VT-hist
#| include: false
#| fig-asp: 0.5
#| fig-cap: Histogram of point visibility testing performance.
#| out-width: "50%"
#| fig-align: "left"

# histograms of visual threshold testing performance 
# not included in final manuscript

ggplot(size_and_contrast_exp_tidy, aes(x = VT_no_correct)) +
  geom_histogram(binwidth = 1,colour = "grey", fill = "darkgrey") +
  theme_ggdist() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_text(size = 14)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 9)) +
  labs(x = "Number of Visual Threshold Tests Correct (out of 6)",
       y = "Count")
```

## Dot Pitch {#sec-dot-pitch}

We employed a method for obtaining the dot pitch of participants' monitors [@screenscale].
Combining this with monitor resolution information allows us to calculate the physical on-screen
size of scatterplot points. Participants were asked to hold a standard size
credit/debit/ID card (ISO/IEC 7810 ID-1) up to their screen and resize an on-screen card
until their sizes matched. We assumed a widescreen 16:9 aspect ratio and calculated
dot pitch based on these measurements. Mean dot pitch was 
`r printnum(dot_pitch$mean_dp, digits = 2)`mm ($SD$ = `r printnum(dot_pitch$sd_dp, digits = 2)`),
corresponding to a physical onscreen size of `r printnum(dot_pitch$mean_dp*13)`mm
on a 1920 x 1080 pixel monitor for the smallest points displayed. 
We include analyses with dot pitch as a fixed effect in @sec-add-analyses.

## Procedure {#sec-gen-procedure}

Both experiments were built using PsychoPy [@pierce_2019] and hosted on Pavlovia.org.
Participants were only permitted to complete the experiment on a desktop or laptop
computer. Each participant was first shown the participant information sheet and provided
consent through key presses in response to consent statements. They were asked to
provide their age in a free text box, followed by their gender identity. Participants
completed the 5-item Subjective Graph Literacy test [@garcia_2016], followed by the
visual threshold task described in @sec-VT and the screen scale task described in 
@sec-dot-pitch. Participants were given instructions, and were then shown examples
of scatterplots with correlations of *r* = 0.2, 0.5, 0.8, and 0.95, as piloting of
a previous experiment indicated some of the lay population may be unfamiliar
with the visual character of scatterplots. @sec-results contains further
analysis of the potential training effects of this. Two practice trials were
given before the experiment began. Participants worked through a randomly
presented series of 180 experimental trials and were asked to
use a slider to estimate correlation to 2 decimal places. Visual masks preceded
each scatterplot. Interspersed were 6 attention check trials which explicitly
asked participants to ignore the scatterplot and set the slider to 0 or 1.

## Participants {#sec-participants}

150 participants were recruited using the Prolific.co platform. Normal to 
corrected-to-normal vision and English fluency were required for participation.
In addition, participants who had completed any of our previous studies
into correlation estimation in scatterplots (references removed for anon) 
were prevented from participating. Data were collected from 158 participants. 
8 failed more than 2 out of 6 attention
check questions, and, as per pre-registration stipulations, were rejected from the
study. Data from the remaining 150 participants were included in the full analysis
(`r printnum(gender$M, digits = 1)`% male, `r printnum(gender$F, digits = 1)`% female,
and `r printnum(gender$NB, digits = 1)`% non-binary).
Participants' mean age was `r printnum(age$mean, digits = 1)` (*SD* = `r printnum(age$sd, digits = 1)`).
Participants' mean graph literacy score was `r printnum(literacy$mean, digits = 1)`
(*SD* = `r printnum(literacy$sd, digits = 1)`). The average
time taken to complete the experiment was 37 minutes (*SD* = 12.3).

## Design {#sec-design}

We used a fully repeated-measures 2*2 factorial design. Each participant
saw each combination of size and contrast decay condition plots for a total of 180
experimental items. Participants viewed these experimental items, along with 6
attention check items, in a fully randomized order. All experimental code, materials,
and instructions are hosted at (link removed for anon).

# Results {#sec-results}

Our first two hypotheses were fully supported in this experiment. The combination of 
standard orientation size and contrast decay functions produced the most accurate estimates of
correlation, although this also resulted in a large correlation overestimation bias for many
values of *r* (see @fig-diff-error-bars-plot). Our second hypothesis was also 
supported; the combination of inverted size and inverted contrast decay conditions
produced the least accurate estimates of correlation. We found no support for our third
hypothesis; there was no significant difference in correlation estimates for
standard orientation size/inverted orientation contrast decay plots and 
inverted orientation size/standard orientation contrast
decay plots (Z = -2.26, *p* = .11), however we did find a significant interaction effect
that provides evidence that the combination of size and contrast decay functions
is not additive in nature.

```{r}
#| label: model
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# r estimation error modeling

model <- buildmer(difference ~ size * contrast +
                       (1 + size * contrast | participant) +
                       (1 + size * contrast | item),
                     data = size_and_contrast_exp_tidy)
```

```{r}
#| label: assign-slot

model <- model@model
```

```{r}
#| label: model-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# buildcomparison model with both fixed effects terms removed

model_cmpr <- comparison(model)
```

```{r}
#| label: anova

# runs ANOVA between experimental and comparison models

anova_results(model, model_cmpr)
```

```{r}
#| label: fig-dot-plot
#| include: false
#| fig-cap: Mean r estimation for each combination of conditions. 95% confidence intervals are shown as error bars.
#| fig-asp: 0.6
#| out-width: 50%
#| fig-align: "left"

# dot plot of mean errors and 95% CIs for each combination of size and contrast decay conditions
# currently not used s- EMM plot used in place

dot_plot_function(size_and_contrast_exp_tidy, "Size Decay : Contrast Decay") +
    scale_x_discrete(labels = labels_size_contrast) +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 9))
```

```{r}
#| label: tbl-sig
#| include: true
#| tbl-cap: Significances of fixed effects and the interaction between them. Semi-partial R^2^ for each fixed effect and the interaction term is also displayed in lieu of effect sizes.

make_sig_table(model)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`). 
Deviation coding was used for each of the experimental factors. We used the **buildmer** and
**lme4** (version 1.1-34 [@lme4]) packages to build a linear mixed effects model where the difference
between objective and rated *r* value was predicted by the size and contrast decay
conditions used. A likelihood ratio test revealed that the model including point size
and contrast decay conditions as fixed effects explained significantly more variance
than the null ($\chi^2$(`r in_paren(model.df)`) = 
`r printnum(model.Chisq)`, *p* `r printp(model.p, add_equals = TRUE)`). There
were significant fixed effects of size decay and contrast decay conditions, as well
as a significant interaction between the two. The experimental model has random intercepts for items
and participants, and a random slope for the size decay factor with regards to participants.
Due both to our use of a linear mixed model with an interaction
term, and our lack of comparative baseline condition (i.e no size or contrast function used),
we do not report a measure of effect size. Instead we report the amounts of variance
explained by each fixed effect term and the interaction term as semi-partial
R^2^ [@nakagawa_2013]. These values were calculated using the **r2glmm** package
(version 0.1.2 [@r2glmm]) and can be see in @tbl-sig along with all model statistics.
The **emmeans** package (version 1.8.8 [@emmeans]) was used to calculate pairwise
comparisons between levels of the size and contrast decay factor, and can be seen
in @tbl-contrasts.

```{r}
#| label: fig-emm-plot
#| include: true
#| fig-asp: 0.4
#| out-width: "100%"
#| fig-cap: Estimated marginal means for each combination of size and contrast decay conditions, including asymptotic lower and upper confidence.

as_tibble(emmeans(model, pairwise ~ size * contrast)[[1]]) %>%
  mutate("size" = recode(size,
                         "S" = "Standard Orientation Size",
                         "I" = "Inverted Orientation Size"),
          "contrast" = recode(contrast,
                         "S" = "Standard Orientation Contrast",
                         "I" = "Inverted Orientation Contrast")) %>%
  ggplot(aes(x = reorder(size:contrast, emmean), y = emmean*-1)) +
  geom_point(size = 1.75) +
  geom_pointrange(aes(ymin = asymp.LCL*-1, ymax = asymp.UCL*-1)) +
  scale_x_discrete(labels = ~ gsub(":", "\n", .x)) +
  theme_ggdist() + 
  labs(y = "Estimated Marginal Mean of Error",
       x = " Size : Contrast") +
  theme(axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
  coord_flip() +
  geom_abline(intercept = 0, slope = 0, linetype = 2) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = -.02,
                   yend = -.08,
           arrow = arrow(length = unit(0.1, "cm" ))) +
  annotate(geom = "segment",
                   x = 4.4,
                   xend = 4.4,
                   y = .02,
                   yend = .08,
           arrow = arrow(length = unit(0.1, "cm"))) +
  annotate(geom = "text",
           x = 4.15,
           y = -.05,
           label = "Underestimation",
           size = 3) +
  annotate(geom = "text",
           x = 4.15,
           y = .05,
           label = "Overestimation",
           size = 3)
  
```

```{r}
#| label: fig-interaction-plot
#| include: false
#| fig-cap: Interaction plot showing the moderating effect of encoding channel on decay orientation.
#| fig-asp: 0.7
#| fig-align: "left"
#| #| out-width: 50%

# not used for manuscript

emm <- emmeans(model, ~ size * contrast)

emm %>%
  as_tibble() %>%
  mutate("contrast" = recode(contrast,
                             "I" = "Inverted Decay",
                             "N" = "Non-linear Decay")) %>%
  ggplot() +
    aes(x = size, y = emmean, colour = contrast) +
    geom_line(aes(group = contrast), size = 1) +
    geom_point(size = 3) +
    theme_ggdist() +
    ylim(-0.1,0.175) +
    scale_x_discrete(labels = c("Non-Linear Decay", "Inverted Decay")) +
    labs(x = "Size Decay Condition",
         y = "Estimated Marginal Mean",
         colour = "Contrast\nDecay\nCondition") +
  theme(legend.position = c(0.75,0.3),
        axis.text = element_text(size = 14),
        axis.title = element_text(size = 16))
```

```{r}
#| label: tbl-contrasts
#| include: true
#| tbl-cap: Pairwise comparisons. SO = Standard Orientation, IO = Inverted Orientation. The interaction is driven by the non-additive nature of combining point size and contrast decay functions, and the only nonsignificant contrast is found when incongruent decay functions are tested.

table_df <- contrasts_extract(model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  rename("\\textit{p}" = "p.value") %>%
  mutate('Contrast' = recode(Contrast,
         "S I - I I" = "SO Size x IO Contrast <-> IO Size x IO Contrast",
         "S I - S S" = "SO Size x IO Contrast <-> SO Size x SO Contrast",
         "S I - I S" = "SO Size x IO Contrast <-> IO Size x SO Contrast",
         "I I - S S" = "IO Size x IO Contrast <-> SO Size x SO Contrast",
         "I I - I S" = "IO Size x IO Contrast <-> IO Size x SO Contrast",
         "S S - I S" = "SO Size x SO Contrast <-> IO Size x SO Contrast"))

kable(table_df, booktabs = TRUE, digits = c(0,2,3), escape = FALSE)
```

```{r}
#| label: fig-diff-error-bars-plot
#| include: true
#| out-width: 100%
#| fig-cap: Plots showing how participants' correlation estimation errors change as a function of the *r* value for each combination of size and contrast decay factors.
#| fig-asp: 1

plot_error_bars_function(size_and_contrast_exp_tidy %>%
                        mutate(condition_abs = fct_relevel(condition_abs,
                                                          c("A", "B", "X", "Y"))),
                        "difference",
                        labels_size_contrast) +
  geom_hline(yintercept = 0, linetype = 2)
```

```{r}
#| label: additional-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

lit_model <- lmer(add.terms(formula(model), "literacy"),
                  data = size_and_contrast_exp_tidy)

anova_results(lit_model, model)

VT_model <- lmer(add.terms(formula(model), "VT_no_correct"),
                  data = size_and_contrast_exp_tidy)

anova_results(VT_model, model)

dot_pitch_model <- lmer(add.terms(formula(model), "dot_pitch"),
                        data = size_and_contrast_exp_tidy)

anova_results(dot_pitch_model, model)
```

## Additional Analyses {#sec-add-analyses}

We find no effects of graph literacy ($\chi^2$(`r in_paren(lit_model.df)`) = 
`r printnum(lit_model.Chisq)`, *p* `r printp(lit_model.p, add_equals = TRUE)`),
performance on the visual threshold task ($\chi^2$(`r in_paren(VT_model.df)`) = 
`r printnum(VT_model.Chisq)`, *p* `r printp(VT_model.p, add_equals = TRUE)`), or
dot pitch ($\chi^2$(`r in_paren(dot_pitch_model.df)`) = `r printnum(dot_pitch_model.Chisq)`,
*p* `r printp(dot_pitch_model.p, add_equals = TRUE)`) on participants' errors in correlation estimation.

# Discussion {#sec-discussion}

Our findings here provide further confirmatory evidence of what has been found
previously with regards to the effects of point size and contrast manipulations
on correlation estimation in scatterplots. Namely, that while both manipulations
have a significant effect, the effect of changing point sizes is stronger, and that
while we can influence correlation estimates in either direction, standard
orientation manipulations are more powerful than inverted ones [@strain_2023; @strain_2023b].
As one would expect, we also see an effect of orientation congruency on the extent
to which a manipulation can bias correlation estimates. 
The lack of support for our third hypothesis, that there would be a
difference in correlation estimates between incongruent conditions, was surprising
given the greater strength of the size channel relative to contrast demonstrated
in previous work [@strain_2023; @strain_2023b]. Despite the lack of support for
this hypothesis, we did find that the size decay channel explained more variance (.104)
in our model than contrast decay (.087) was able to.

## Combining Manipulations {#sec-combining}

@fig-emm-plot and @fig-diff-error-bars-plot show how, on average, 
the combination of standard orientation size and contrast decay conditions
has resulted in an overestimation of *r* for the majority of values.
While this does not directly solve the underestimation problem, it does demonstrate
that with regards to using point size and contrast to bias viewer's estimates
of correlation in scatterplots, there would appear to be few limitations. The issue
here is not one of our ability to change people's perceptions, but of *tuning* the
use of these visual factors to be able to change people's perceptions in systematic
ways. We explore what further work would need to be done to do this in @sec-future-work.
Combining inverted size and contrast decay functions also had the predicted effect
in this case, producing the lowest and least accurate estimates of correlation.
Combining inverted manipulations did not, however, significantly change the shape
of the estimation curve (see @fig-diff-error-bars-plot). In addition to interacting
non-additively, the effects we observe operate differently depending on the direction
of the change induced in perception. This finding can also explain the lack of support
found for our hypothesis that there would be a significant difference
in *r* estimation error between the two incongruent conditions. Despite the size channel
being more powerful with regards to influencing correlation estimates, the fact
that this power depends on the direction the function is set causes incongruent
functions to act against each other in ways we would not expect. Indeed, the 
incongruent condition that used a standard orientation size decay function did 
exhibit lower mean error than the one using inverted size decay (see @fig-diff-error-bars-plot),
however in each case the contrast decay appears to have blunted the power of the
size decay function to the extent that the difference in errors is not statistically
significant.

## Estimation Precision

Much previous work is conclusive with regards to the finding that *r* estimation
precision increases with the objective *r* value [@rensink_2010; @rensink_2012; @rensink_2014;
@rensink_2017; @doherty_2007]. More recent work using size or contrast decay
functions similar to the ones used here [@strain_2023; @strain_2023b] has found
that in some cases, precision in *r* estimation is constant across the range of
*r* values investigated. For example, the use of a size decay function, whether
using standard/inverted orientation non-linear functions or a linear decay
function, results in **no** change in *r* estimation precision [@strain_2023b].
When contrast is used in the same ways, only an inverted decay function **does not**
exhibit the conventional increase in precision with *r*. In the present work,
precision in *r* estimation increased whenever a standard orientation contrast
decay function was used.

## Contributions of Size and Contrast Decay

Incorporating data from previous work [@strain_2023; @strain_2023b] that used 
similar decay functions and experimental paradigms allows us to compare estimation
curves for size decay and contrast decay in isolation and in combination. 
@fig-est-multi-exp shows correlation estimation error curves in the present experiment
and in two previous studies that used decay functions applied solely to size or
contrast.

```{r}
#| label: fig-est-multi-exp
#| include: true
#| out-width: "100%"
#| fig-asp: 0.33
#| fig-cap: Plotting *r* estimation error against the objective *r* value for contrast and size decay in isolation from previous work, and for their combination in the present study. The dashed line represents 0 error in correlation estimation, and standard deviations are shown as error bars. Note that these curves have been smoothed.

# dataframe containing values from previous work and the current is included
# in the data folder 

facet_order <- c("contrast_manipulated", "size_manipulated", "additive_manipulation")

all_exp_df <- read_csv("data/all_exp.csv") %>%
    drop_na() %>%
    filter(factor != "standard_plot") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>%
    mutate(factor = factor(factor, levels = facet_order))

  all_exp_df %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 4, labeller = labeller(factor = labels_all_exp)) +
    labs(x = "Objective r",
         y = "Mean r Estimation Error") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)

```

Using contrast decay alone does not significantly change the shape of the estimation
error curve, whereas using size decay does. When size and contrast decay functions are combined,
however, the shape appears similar to that of size. This is in line with
previous work establishing size as a more potent channel for the manipulation
of correlation estimates [@strain_2023b]. It would appear that the addition of
the contrast curve moderates the effect of the size curve as a function of the
objective *r* value itself, without affecting the general shape of the curve.
In the following, we briefly discuss the effects of each manipulation in isolation
and the manipulations together, before making a case for the inclusion of both
when tuning scatterplots for correlation estimation due to the complementary
benefits each confers. Throughout the course of this section it should be noted
that the analyses include data from several separate experiments. We argue that
their methodological similarities render comparison appropriate, but we acknowledge
the potential for overstated conclusions.

Using a contrast decay function in isolation has a small effect on
correlation estimation. It does little to change the shape of the underestimation
curve (see @fig-underestimation-curve), but slightly biases *r* estimates up to
partially correct for the underestimation observed with standard scatterplots [@strain_2023].
Importantly, it also preserves the increase in correlation estimation precision 
that we would expect to find during correlation estimation tasks. Using the size
decay function in isolation has a more dramatic effect. The shape of the estimation
curve is altered quite radically, and there is no increase in precision
with the objective *r* value [@strain_2023b]. Size decay over-corrects at lower values of *r*,
leading to an overestimation effect, while at high values the curve begins to 
change direction, leading to a more severe underestimation. In the middle range
of *r* values, however, the size decay function in isolation performs well. One option
for tuning correlation estimation using these functions would therefore be to 
simply use the size decay function while rapidly reducing its severity outside of 0.3 < *r* < 0.8.
Used together however, we can exploit the power of the size decay function whilst maintaining
the expected increase in precision with *r* that the contrast decay function confers.
It is clear that the simple combination we have used in the present study does
not represent an ideal tuning, as participants overestimated *r* for the majority of 
values, but this confirms that there is the scope to bias *r* estimates significantly
using the functions supplied here. Further work would be required to obtain 
precise measures of the power of each decay function and their power together. Doing
this would allow us to tune each function according to both objective *r* value
and the tuning of the other function to produce accurate correlation estimates.
We can also derive new curves that describe the effect
that each manipulation and the combination of manipulations has on people's
estimates of correlation (@fig-power-plot) by comparing them to estimates without
any manipulation present. We term this 'power'. The dotted line on each plot
shows the power we would need to correct for the standard underestimation
curve (see @fig-underestimation-curve). As we can see, size alone provides the 
closest to the requirement, and combining size and contrast decay functions 
results in gross overestimation.

```{r}
#| label: fig-power-plot
#| include: true
#| out-width: "100%"
#| fig-cap: Power is the difference between what is observed when a decay function/combination of decay functions is used and what is observed when no manipulation is used. The dashed line represents the power that would be required to correct for the underestimation of correlation in scatterplots.

# dataframe containing values from previous experiments and the current is included
# in the data folder 

curves <- read_csv("data/curves_df.csv") 

facet_order <- c("contrast_power", "size_power", "additive_power")

# create necessary power curve

necessary_power <- curves %>% select(c("my_rs", "standard_curve")) %>%
  mutate(mirror = my_rs-standard_curve)

 curves %>% 
    drop_na() %>%
    select(c("contrast_power",
             "size_power",
             "additive_power",
             "my_rs")) %>%
    pivot_longer(cols = c("contrast_power",
                          "size_power",
                          "additive_power"),
                 names_to = "factor", values_to = "power") %>%
    mutate(factor = factor(factor, levels = facet_order)) %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(power), mean = mean(power)) %>% 
    ggplot(aes(x = my_rs, y = mean)) + 
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 8)) +
    facet_wrap(factor ~., ncol = 5, labeller = labeller(factor = labels_power)) +
    labs(x = "Objective r",
         y = "Power") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    geom_smooth(data = necessary_power,
                aes(x = my_rs, y = mirror),
                se = F,
                linetype = 2,
                colour = "black",
                size = 0.4) +
    xlim(0.2,1) +
    ylim(-0.1,0.3)

```

## Mechanisms {#sec-mechs}

Previous work has made the case for contrast and size decay acting primarily 
through salience/perceptual weighting, with the caveat that spatial certainty
also plays a small part in the mechanism behind size decay [@strain_2023; @strain_2023b].
Our results are supportive of this notion, with our lowest and highest estimates 
being observed in the incongruent and congruent conditions respectively. These
findings also support dot density [@yang_2023] and feature-based attentional bias accounts
[@hong_2021; @sun_2016]. As all of these mechanisms would be expected to operate
in the same direction, making conclusions about the relative contributions of
each is difficult. We argue that in this case, making these conclusions 
at all is unnecessary. The body of evidence generally points to a high-level probability
density account [@rensink_2017; @rensink_2022]. On a lower level, numerous candidate
mechanisms exist, which are mostly expected to act in the same direction. Previous
work concluded that spatial certainty [@strain_2023b] may play a small role with 
regards to the effects of size decay on correlation perception; our results
neither confirm nor refute this, but instead provide further evidence for
salience/perceptual weighting/dot density changing participants' perceptions
of probability density to affect correlation estimates.

## Limitations {#sec-limitations}

Firstly, our participants' performance on the point visibility task was poor, with
an average score of only `r printnum(VT$mean_VT_perc_correct)`%.
It is clear from these results that for many participants, the smallest and lowest
contrast points we used were simply not visible, although it would seem that this
low visibility had no significant effect on correlation estimates. Regardless,
for many of our participants it will have appeared that we were removing data, which
goes against our intended aims. Solving this would require a by-participant calibration
of point size and point contrast, which is beyond the scope of our current methodology,
as it would require stimuli to be fully re-generated for each participant. We aim to
implement this in the future, although it would require a different platform,
such as a server-based instance of Rstudio being run in the background to re-generate
stimuli per a calibration task completed by the participant. We cannot say precisely
what proportions of the observed effect in the standard orientation congruent condition
were due to size or contrast decay. We can conclude that these effects are not linearly
additive, but must suggest further work to define precisely each of their contributions.

## Future Work {#sec-future-work}

There is evidence that viewers overestimate correlation in negatively correlated
scatterplots [@sher_2017]. Future work may make use of the functions and findings
we have provided to begin solving this problem. We found evidence that the
influence of size and contrast decay functions changes according to the direction
they are operating in, meaning experimental work with negatively correlated
scatterplots would be required, and results may differ significantly from
findings related to the *underestimation* of correlation in *positively*
correlated scatterplots. For size and contrast decay in the present work we used equation 1. 
Given our finding that the combination is non-additive, there are a multitude of parameters
that could be adjusted for each decay function that require rigorous testing
in order to produce concrete values of the contributions of each. The value of 
*b* is one such parameter. We used the same value of *b* (0.25) as in previous
work [@strain_2023; @strain_2023b]. Changing this value can increase or decrease
the severity of the effect in question. Additionally, we used a constant and a scaling factor
with the size decay manipulation to ensure our points were visible. These values
could also be changed. Aside from changing aspects of equation 1, there are other equations
that could be used, including ones that take into account objective *r* value to
change the values used to set point size or contrast. The present work opens the door
for this future work, as it provides the necessary additional data to 
previous work using only size [@strain_2023b] or contrast [@strain_2023] decay functions.
Further testing of these manipulations, both in isolation and in combination using
different decay function parameters will allow researchers to build a more complete
picture of how these visual features impact correlation estimation, and how we
can exploit them to correct for a historic bias.

Through this work we also provide an example of an experimental framework
that we argue should be employed to test a wider range of data visualizations,
statistical summaries, and task types. Our framework is fully open source, and
can be easily adapted for other charts and modalities. Doing this furthers the 
cause of empirically-informed data visualization design.

# References {-}

